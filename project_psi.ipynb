{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project_AI_AGH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries. Using keras to build model with backend tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\minhn\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import dateutil\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import numpy\n",
    "from numpy import ndarray\n",
    "import sys\n",
    "import keras\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read all images_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_params = pandas.read_csv(\"NAPS_valence_arousal_2014.csv\", sep=\";\", header=0, decimal=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Category</th>\n",
       "      <th>Nr</th>\n",
       "      <th>V_H</th>\n",
       "      <th>Description</th>\n",
       "      <th>Valence</th>\n",
       "      <th>Arousal,,</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Animals_001_h</td>\n",
       "      <td>Animals</td>\n",
       "      <td>1</td>\n",
       "      <td>h</td>\n",
       "      <td>Dead Stork</td>\n",
       "      <td>2.57</td>\n",
       "      <td>6.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Animals_002_v</td>\n",
       "      <td>Animals</td>\n",
       "      <td>2</td>\n",
       "      <td>v</td>\n",
       "      <td>Lion</td>\n",
       "      <td>6.24</td>\n",
       "      <td>6.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Animals_003_h</td>\n",
       "      <td>Animals</td>\n",
       "      <td>3</td>\n",
       "      <td>h</td>\n",
       "      <td>Snake</td>\n",
       "      <td>5.24</td>\n",
       "      <td>5.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Animals_004_v</td>\n",
       "      <td>Animals</td>\n",
       "      <td>4</td>\n",
       "      <td>v</td>\n",
       "      <td>Wolf</td>\n",
       "      <td>4.50</td>\n",
       "      <td>7.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Animals_005_h</td>\n",
       "      <td>Animals</td>\n",
       "      <td>5</td>\n",
       "      <td>h</td>\n",
       "      <td>Bat</td>\n",
       "      <td>5.31</td>\n",
       "      <td>5.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Animals_006_v</td>\n",
       "      <td>Animals</td>\n",
       "      <td>6</td>\n",
       "      <td>v</td>\n",
       "      <td>Snake</td>\n",
       "      <td>5.13</td>\n",
       "      <td>6.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Animals_007_h</td>\n",
       "      <td>Animals</td>\n",
       "      <td>7</td>\n",
       "      <td>h</td>\n",
       "      <td>Wolf</td>\n",
       "      <td>4.76</td>\n",
       "      <td>7.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Animals_008_v</td>\n",
       "      <td>Animals</td>\n",
       "      <td>8</td>\n",
       "      <td>v</td>\n",
       "      <td>Fighting Chickens</td>\n",
       "      <td>2.63</td>\n",
       "      <td>6.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Animals_009_v</td>\n",
       "      <td>Animals</td>\n",
       "      <td>9</td>\n",
       "      <td>v</td>\n",
       "      <td>Cat</td>\n",
       "      <td>5.79</td>\n",
       "      <td>5.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Animals_010_h</td>\n",
       "      <td>Animals</td>\n",
       "      <td>10</td>\n",
       "      <td>h</td>\n",
       "      <td>Sick Kitten</td>\n",
       "      <td>4.59</td>\n",
       "      <td>5.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Animals_011_h</td>\n",
       "      <td>Animals</td>\n",
       "      <td>11</td>\n",
       "      <td>h</td>\n",
       "      <td>Black Panther</td>\n",
       "      <td>5.00</td>\n",
       "      <td>7.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Animals_012_h</td>\n",
       "      <td>Animals</td>\n",
       "      <td>12</td>\n",
       "      <td>h</td>\n",
       "      <td>Fighting Dogs</td>\n",
       "      <td>3.62</td>\n",
       "      <td>6.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Animals_013_h</td>\n",
       "      <td>Animals</td>\n",
       "      <td>13</td>\n",
       "      <td>h</td>\n",
       "      <td>Dead Deer</td>\n",
       "      <td>2.90</td>\n",
       "      <td>6.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Animals_014_h</td>\n",
       "      <td>Animals</td>\n",
       "      <td>14</td>\n",
       "      <td>h</td>\n",
       "      <td>Snake</td>\n",
       "      <td>5.14</td>\n",
       "      <td>6.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Animals_015_h</td>\n",
       "      <td>Animals</td>\n",
       "      <td>15</td>\n",
       "      <td>h</td>\n",
       "      <td>Snake</td>\n",
       "      <td>4.57</td>\n",
       "      <td>5.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Animals_016_h</td>\n",
       "      <td>Animals</td>\n",
       "      <td>16</td>\n",
       "      <td>h</td>\n",
       "      <td>Dead Hamster</td>\n",
       "      <td>2.67</td>\n",
       "      <td>6.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Animals_017_v</td>\n",
       "      <td>Animals</td>\n",
       "      <td>17</td>\n",
       "      <td>v</td>\n",
       "      <td>Dead Bird</td>\n",
       "      <td>3.23</td>\n",
       "      <td>5.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Animals_018_h</td>\n",
       "      <td>Animals</td>\n",
       "      <td>18</td>\n",
       "      <td>h</td>\n",
       "      <td>Dead Frog</td>\n",
       "      <td>3.34</td>\n",
       "      <td>6.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Animals_019_h</td>\n",
       "      <td>Animals</td>\n",
       "      <td>19</td>\n",
       "      <td>h</td>\n",
       "      <td>Dead Kangaroo</td>\n",
       "      <td>2.68</td>\n",
       "      <td>6.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Animals_020_h</td>\n",
       "      <td>Animals</td>\n",
       "      <td>20</td>\n",
       "      <td>h</td>\n",
       "      <td>Alligator</td>\n",
       "      <td>5.34</td>\n",
       "      <td>6.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Animals_021_h</td>\n",
       "      <td>Animals</td>\n",
       "      <td>21</td>\n",
       "      <td>h</td>\n",
       "      <td>Small Alligators</td>\n",
       "      <td>4.60</td>\n",
       "      <td>6.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Animals_022_h</td>\n",
       "      <td>Animals</td>\n",
       "      <td>22</td>\n",
       "      <td>h</td>\n",
       "      <td>Small Alligators</td>\n",
       "      <td>5.74</td>\n",
       "      <td>5.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Animals_023_h</td>\n",
       "      <td>Animals</td>\n",
       "      <td>23</td>\n",
       "      <td>h</td>\n",
       "      <td>Dog'S Teeth</td>\n",
       "      <td>4.36</td>\n",
       "      <td>6.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Animals_024_h</td>\n",
       "      <td>Animals</td>\n",
       "      <td>24</td>\n",
       "      <td>h</td>\n",
       "      <td>Dead Cat</td>\n",
       "      <td>2.44</td>\n",
       "      <td>6.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Animals_025_h</td>\n",
       "      <td>Animals</td>\n",
       "      <td>25</td>\n",
       "      <td>h</td>\n",
       "      <td>Sheep</td>\n",
       "      <td>2.65</td>\n",
       "      <td>6.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Animals_026_h</td>\n",
       "      <td>Animals</td>\n",
       "      <td>26</td>\n",
       "      <td>h</td>\n",
       "      <td>Snake</td>\n",
       "      <td>5.35</td>\n",
       "      <td>5.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Animals_027_h</td>\n",
       "      <td>Animals</td>\n",
       "      <td>27</td>\n",
       "      <td>h</td>\n",
       "      <td>Dead Bird</td>\n",
       "      <td>2.55</td>\n",
       "      <td>6.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Animals_028_h</td>\n",
       "      <td>Animals</td>\n",
       "      <td>28</td>\n",
       "      <td>h</td>\n",
       "      <td>Bat'S Teeth</td>\n",
       "      <td>5.55</td>\n",
       "      <td>6.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Animals_029_v</td>\n",
       "      <td>Animals</td>\n",
       "      <td>29</td>\n",
       "      <td>v</td>\n",
       "      <td>Horse Legs</td>\n",
       "      <td>3.10</td>\n",
       "      <td>6.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Animals_030_h</td>\n",
       "      <td>Animals</td>\n",
       "      <td>30</td>\n",
       "      <td>h</td>\n",
       "      <td>Snake</td>\n",
       "      <td>4.74</td>\n",
       "      <td>6.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1326</th>\n",
       "      <td>People_221_h</td>\n",
       "      <td>People</td>\n",
       "      <td>221</td>\n",
       "      <td>h</td>\n",
       "      <td>Surgery</td>\n",
       "      <td>1.88</td>\n",
       "      <td>7.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1327</th>\n",
       "      <td>People_222_h</td>\n",
       "      <td>People</td>\n",
       "      <td>222</td>\n",
       "      <td>h</td>\n",
       "      <td>Hooked Skin</td>\n",
       "      <td>2.17</td>\n",
       "      <td>7.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1328</th>\n",
       "      <td>People_223_h</td>\n",
       "      <td>People</td>\n",
       "      <td>223</td>\n",
       "      <td>h</td>\n",
       "      <td>Surgery</td>\n",
       "      <td>3.18</td>\n",
       "      <td>6.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1329</th>\n",
       "      <td>People_224_h</td>\n",
       "      <td>People</td>\n",
       "      <td>224</td>\n",
       "      <td>h</td>\n",
       "      <td>Surgery</td>\n",
       "      <td>3.49</td>\n",
       "      <td>6.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1330</th>\n",
       "      <td>People_225_h</td>\n",
       "      <td>People</td>\n",
       "      <td>225</td>\n",
       "      <td>h</td>\n",
       "      <td>Dead Body</td>\n",
       "      <td>2.50</td>\n",
       "      <td>6.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1331</th>\n",
       "      <td>People_226_h</td>\n",
       "      <td>People</td>\n",
       "      <td>226</td>\n",
       "      <td>h</td>\n",
       "      <td>Accident</td>\n",
       "      <td>1.81</td>\n",
       "      <td>7.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1332</th>\n",
       "      <td>People_227_h</td>\n",
       "      <td>People</td>\n",
       "      <td>227</td>\n",
       "      <td>h</td>\n",
       "      <td>Burns</td>\n",
       "      <td>1.86</td>\n",
       "      <td>7.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1333</th>\n",
       "      <td>People_228_h</td>\n",
       "      <td>People</td>\n",
       "      <td>228</td>\n",
       "      <td>h</td>\n",
       "      <td>Surgery</td>\n",
       "      <td>3.71</td>\n",
       "      <td>6.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1334</th>\n",
       "      <td>People_229_h</td>\n",
       "      <td>People</td>\n",
       "      <td>229</td>\n",
       "      <td>h</td>\n",
       "      <td>Surgery</td>\n",
       "      <td>3.90</td>\n",
       "      <td>6.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1335</th>\n",
       "      <td>People_230_h</td>\n",
       "      <td>People</td>\n",
       "      <td>230</td>\n",
       "      <td>h</td>\n",
       "      <td>Surgery</td>\n",
       "      <td>3.43</td>\n",
       "      <td>6.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1336</th>\n",
       "      <td>People_231_h</td>\n",
       "      <td>People</td>\n",
       "      <td>231</td>\n",
       "      <td>h</td>\n",
       "      <td>Skeleton</td>\n",
       "      <td>4.10</td>\n",
       "      <td>6.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1337</th>\n",
       "      <td>People_232_h</td>\n",
       "      <td>People</td>\n",
       "      <td>232</td>\n",
       "      <td>h</td>\n",
       "      <td>Surgery</td>\n",
       "      <td>3.63</td>\n",
       "      <td>6.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1338</th>\n",
       "      <td>People_233_h</td>\n",
       "      <td>People</td>\n",
       "      <td>233</td>\n",
       "      <td>h</td>\n",
       "      <td>Dead Animal</td>\n",
       "      <td>2.47</td>\n",
       "      <td>6.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1339</th>\n",
       "      <td>People_234_h</td>\n",
       "      <td>People</td>\n",
       "      <td>234</td>\n",
       "      <td>h</td>\n",
       "      <td>Surgery</td>\n",
       "      <td>4.06</td>\n",
       "      <td>6.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1340</th>\n",
       "      <td>People_235_h</td>\n",
       "      <td>People</td>\n",
       "      <td>235</td>\n",
       "      <td>h</td>\n",
       "      <td>Dead Body</td>\n",
       "      <td>2.67</td>\n",
       "      <td>6.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1341</th>\n",
       "      <td>People_236_v</td>\n",
       "      <td>People</td>\n",
       "      <td>236</td>\n",
       "      <td>v</td>\n",
       "      <td>Skeletons</td>\n",
       "      <td>4.38</td>\n",
       "      <td>5.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1342</th>\n",
       "      <td>People_237_h</td>\n",
       "      <td>People</td>\n",
       "      <td>237</td>\n",
       "      <td>h</td>\n",
       "      <td>Face Skin</td>\n",
       "      <td>1.60</td>\n",
       "      <td>7.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1343</th>\n",
       "      <td>People_238_h</td>\n",
       "      <td>People</td>\n",
       "      <td>238</td>\n",
       "      <td>h</td>\n",
       "      <td>Dead Bodies</td>\n",
       "      <td>1.33</td>\n",
       "      <td>7.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1344</th>\n",
       "      <td>People_239_h</td>\n",
       "      <td>People</td>\n",
       "      <td>239</td>\n",
       "      <td>h</td>\n",
       "      <td>Skin Disease</td>\n",
       "      <td>2.40</td>\n",
       "      <td>6.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1345</th>\n",
       "      <td>People_240_h</td>\n",
       "      <td>People</td>\n",
       "      <td>240</td>\n",
       "      <td>h</td>\n",
       "      <td>Skin Disease</td>\n",
       "      <td>1.82</td>\n",
       "      <td>7.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1346</th>\n",
       "      <td>People_241_h</td>\n",
       "      <td>People</td>\n",
       "      <td>241</td>\n",
       "      <td>h</td>\n",
       "      <td>Eye Disease</td>\n",
       "      <td>2.52</td>\n",
       "      <td>6.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1347</th>\n",
       "      <td>People_242_v</td>\n",
       "      <td>People</td>\n",
       "      <td>242</td>\n",
       "      <td>v</td>\n",
       "      <td>Dead Sheep</td>\n",
       "      <td>2.86</td>\n",
       "      <td>7.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1348</th>\n",
       "      <td>People_243_h</td>\n",
       "      <td>People</td>\n",
       "      <td>243</td>\n",
       "      <td>h</td>\n",
       "      <td>Eye Disease</td>\n",
       "      <td>2.84</td>\n",
       "      <td>6.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1349</th>\n",
       "      <td>People_244_v</td>\n",
       "      <td>People</td>\n",
       "      <td>244</td>\n",
       "      <td>v</td>\n",
       "      <td>Dentist</td>\n",
       "      <td>3.56</td>\n",
       "      <td>6.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1350</th>\n",
       "      <td>People_245_v</td>\n",
       "      <td>People</td>\n",
       "      <td>245</td>\n",
       "      <td>v</td>\n",
       "      <td>Skin Disease</td>\n",
       "      <td>3.42</td>\n",
       "      <td>5.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1351</th>\n",
       "      <td>People_246_h</td>\n",
       "      <td>People</td>\n",
       "      <td>246</td>\n",
       "      <td>h</td>\n",
       "      <td>Black Eye</td>\n",
       "      <td>1.96</td>\n",
       "      <td>7.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1352</th>\n",
       "      <td>People_247_v</td>\n",
       "      <td>People</td>\n",
       "      <td>247</td>\n",
       "      <td>v</td>\n",
       "      <td>Ear Puncture</td>\n",
       "      <td>4.76</td>\n",
       "      <td>6.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1353</th>\n",
       "      <td>People_248_h</td>\n",
       "      <td>People</td>\n",
       "      <td>248</td>\n",
       "      <td>h</td>\n",
       "      <td>Jet Planes</td>\n",
       "      <td>6.04</td>\n",
       "      <td>5.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1354</th>\n",
       "      <td>People_249_h</td>\n",
       "      <td>People</td>\n",
       "      <td>249</td>\n",
       "      <td>h</td>\n",
       "      <td>Airplane</td>\n",
       "      <td>5.64</td>\n",
       "      <td>5.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1355</th>\n",
       "      <td>People_250_h</td>\n",
       "      <td>People</td>\n",
       "      <td>250</td>\n",
       "      <td>h</td>\n",
       "      <td>Boat</td>\n",
       "      <td>6.76</td>\n",
       "      <td>2.98</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1356 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ID Category   Nr V_H        Description  Valence  Arousal,,\n",
       "0     Animals_001_h  Animals    1   h         Dead Stork     2.57       6.44\n",
       "1     Animals_002_v  Animals    2   v               Lion     6.24       6.68\n",
       "2     Animals_003_h  Animals    3   h              Snake     5.24       5.52\n",
       "3     Animals_004_v  Animals    4   v               Wolf     4.50       7.20\n",
       "4     Animals_005_h  Animals    5   h                Bat     5.31       5.82\n",
       "5     Animals_006_v  Animals    6   v              Snake     5.13       6.23\n",
       "6     Animals_007_h  Animals    7   h               Wolf     4.76       7.60\n",
       "7     Animals_008_v  Animals    8   v  Fighting Chickens     2.63       6.80\n",
       "8     Animals_009_v  Animals    9   v                Cat     5.79       5.61\n",
       "9     Animals_010_h  Animals   10   h        Sick Kitten     4.59       5.90\n",
       "10    Animals_011_h  Animals   11   h      Black Panther     5.00       7.35\n",
       "11    Animals_012_h  Animals   12   h      Fighting Dogs     3.62       6.78\n",
       "12    Animals_013_h  Animals   13   h          Dead Deer     2.90       6.73\n",
       "13    Animals_014_h  Animals   14   h              Snake     5.14       6.00\n",
       "14    Animals_015_h  Animals   15   h              Snake     4.57       5.90\n",
       "15    Animals_016_h  Animals   16   h       Dead Hamster     2.67       6.92\n",
       "16    Animals_017_v  Animals   17   v          Dead Bird     3.23       5.54\n",
       "17    Animals_018_h  Animals   18   h          Dead Frog     3.34       6.17\n",
       "18    Animals_019_h  Animals   19   h      Dead Kangaroo     2.68       6.40\n",
       "19    Animals_020_h  Animals   20   h          Alligator     5.34       6.43\n",
       "20    Animals_021_h  Animals   21   h   Small Alligators     4.60       6.27\n",
       "21    Animals_022_h  Animals   22   h   Small Alligators     5.74       5.75\n",
       "22    Animals_023_h  Animals   23   h        Dog'S Teeth     4.36       6.10\n",
       "23    Animals_024_h  Animals   24   h           Dead Cat     2.44       6.75\n",
       "24    Animals_025_h  Animals   25   h              Sheep     2.65       6.63\n",
       "25    Animals_026_h  Animals   26   h              Snake     5.35       5.88\n",
       "26    Animals_027_h  Animals   27   h          Dead Bird     2.55       6.34\n",
       "27    Animals_028_h  Animals   28   h        Bat'S Teeth     5.55       6.10\n",
       "28    Animals_029_v  Animals   29   v         Horse Legs     3.10       6.13\n",
       "29    Animals_030_h  Animals   30   h              Snake     4.74       6.33\n",
       "...             ...      ...  ...  ..                ...      ...        ...\n",
       "1326   People_221_h   People  221   h            Surgery     1.88       7.57\n",
       "1327   People_222_h   People  222   h        Hooked Skin     2.17       7.63\n",
       "1328   People_223_h   People  223   h            Surgery     3.18       6.61\n",
       "1329   People_224_h   People  224   h            Surgery     3.49       6.56\n",
       "1330   People_225_h   People  225   h          Dead Body     2.50       6.82\n",
       "1331   People_226_h   People  226   h           Accident     1.81       7.40\n",
       "1332   People_227_h   People  227   h              Burns     1.86       7.18\n",
       "1333   People_228_h   People  228   h            Surgery     3.71       6.69\n",
       "1334   People_229_h   People  229   h            Surgery     3.90       6.80\n",
       "1335   People_230_h   People  230   h            Surgery     3.43       6.60\n",
       "1336   People_231_h   People  231   h           Skeleton     4.10       6.16\n",
       "1337   People_232_h   People  232   h            Surgery     3.63       6.42\n",
       "1338   People_233_h   People  233   h        Dead Animal     2.47       6.92\n",
       "1339   People_234_h   People  234   h            Surgery     4.06       6.38\n",
       "1340   People_235_h   People  235   h          Dead Body     2.67       6.57\n",
       "1341   People_236_v   People  236   v          Skeletons     4.38       5.98\n",
       "1342   People_237_h   People  237   h          Face Skin     1.60       7.39\n",
       "1343   People_238_h   People  238   h        Dead Bodies     1.33       7.54\n",
       "1344   People_239_h   People  239   h       Skin Disease     2.40       6.66\n",
       "1345   People_240_h   People  240   h       Skin Disease     1.82       7.50\n",
       "1346   People_241_h   People  241   h        Eye Disease     2.52       6.90\n",
       "1347   People_242_v   People  242   v         Dead Sheep     2.86       7.40\n",
       "1348   People_243_h   People  243   h        Eye Disease     2.84       6.82\n",
       "1349   People_244_v   People  244   v            Dentist     3.56       6.42\n",
       "1350   People_245_v   People  245   v       Skin Disease     3.42       5.90\n",
       "1351   People_246_h   People  246   h          Black Eye     1.96       7.11\n",
       "1352   People_247_v   People  247   v       Ear Puncture     4.76       6.20\n",
       "1353   People_248_h   People  248   h         Jet Planes     6.04       5.83\n",
       "1354   People_249_h   People  249   h           Airplane     5.64       5.67\n",
       "1355   People_250_h   People  250   h               Boat     6.76       2.98\n",
       "\n",
       "[1356 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check images_parasm (NAPS****.csv)\n",
    "images_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define necessary functions for timeStamp and to clear prefix, suffix of file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Provided by example\n",
    "def convert_psychopy_timestamp(timestamp_string):\n",
    "    dt = dateutil.parser.parse(timestamp_string)  # curr time\n",
    "    epoch = datetime.datetime.utcfromtimestamp(0) # timestamp 0\n",
    "    ts = (dt - epoch).total_seconds() * 1000.0    # get time difference in milliseconds\n",
    "    return ts\n",
    "\n",
    "#Provided by example\n",
    "def clear_filename(filename):\n",
    "    start_offset = len(\"pictures\\\\day1\\\\\")\n",
    "    end_offset = len(\".jpg\")\n",
    "    return filename[start_offset:][:-end_offset]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define functions which find all csv file in a directory, function which finds all directories, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding all csv file following the path of provided directory\n",
    "def find_csv_filenames( path_to_dir, suffix=\".csv\" ):\n",
    "    filenames = listdir(path_to_dir)\n",
    "    return [ filename for filename in filenames if filename.endswith( suffix ) ]\n",
    "\n",
    "#Finding all subdirectories following the path of provided directory\n",
    "def find_subdirectories( path_to_dir,):\n",
    "    filenames = listdir(path_to_dir)\n",
    "    return [ filename for filename in filenames if not isfile(join(path_to_dir, filename)) ]\n",
    "\n",
    "#Finding in my_list existing a file which its name starts with my_pattern\n",
    "def find_list_string_startsWith (my_list, my_pattern):\n",
    "    return [x for x in my_list if x.startswith(my_pattern)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters for reading file, declaring all dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_all_participants = \"raw-data\\\\2018-afcai-spring\\\\\"\n",
    "list_all_participants=find_subdirectories(path_to_all_participants)\n",
    "list_all_participants=[person for person in list_all_participants if person != 'C309']\n",
    "\n",
    "#all dictionaries which will be used during processing data\n",
    "#key: research participant. ex: B303, C309\n",
    "my_dictionay_images_params = {}\n",
    "my_dictionary_images_timestamp = {}\n",
    "my_dictionary_HR_mV = {}\n",
    "my_dictionary_HR_bps = {}\n",
    "my_dictionary_HR_nW = {}\n",
    "my_dictionary_GSR = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find GSR for a person and convert all into miliSiemens\n",
    "def find_GSR(path):\n",
    "    list_subDirectories = find_subdirectories(path)\n",
    "    if 'BITalino' in list_subDirectories:\n",
    "        csv_files = find_csv_filenames(path + 'BITalino\\\\')  \n",
    "        csv_file_GSR = find_list_string_startsWith(csv_files, 'GSR')\n",
    "        return pandas.read_csv(path + 'BITalino\\\\' + csv_file_GSR[0], sep=\",\", names=[\"Timestamp\", \"Value\"], skiprows=1)\n",
    "    elif ('Empatica' in  list_subDirectories) and ('eHealth' in list_subDirectories): \n",
    "        csv_files = find_csv_filenames(path + 'eHealth\\\\')\n",
    "        csv_file_BVP = find_list_string_startsWith(csv_files, 'GSR')\n",
    "        data = pandas.read_csv(path + 'eHealth\\\\'  + csv_file_BVP[0], sep=\",\", names=[\"Timestamp\", \"Value\"], skiprows=1)\n",
    "        data.Value = data.Value.apply(func=lambda t: 1000000*(1/t))\n",
    "        return data\n",
    "    else:\n",
    "        subdirectory = list_subDirectories[0]\n",
    "        csv_files = find_csv_filenames(path + subdirectory + '\\\\')\n",
    "        if (subdirectory == 'Empatica'): \n",
    "            csv_file_GSR = find_list_string_startsWith(csv_files, 'GSR')\n",
    "            return pandas.read_csv(path + subdirectory + '\\\\' + csv_file_GSR[0], sep=\";\", names=[\"Timestamp\", \"Value\"])\n",
    "        else:\n",
    "            csv_file_GSR = find_list_string_startsWith(csv_files, 'GSR')\n",
    "            data = pandas.read_csv(path + subdirectory + '\\\\' + csv_file_GSR[0], sep=\",\", names=[\"Timestamp\", \"Value\"], skiprows=1)\n",
    "            data.Value = data.Value.apply(func=lambda t: 1000000*(1/t))\n",
    "            return data\n",
    "\n",
    "#Finding HR and fill it to correspondent dictionary\n",
    "def fill_HR(person, path):\n",
    "    list_subDirectories = find_subdirectories(path)\n",
    "    if 'BITalino' in list_subDirectories:\n",
    "        csv_files = find_csv_filenames(path + 'BITalino\\\\')\n",
    "        \n",
    "        csv_file_BPM = find_list_string_startsWith(csv_files, 'BPM')\n",
    "        my_dictionary_HR_mV[person] = pandas.read_csv(path + 'BITalino\\\\'  + csv_file_BPM[0], sep=\",\", names=[\"Timestamp\", \"Value\"], skiprows=1)\n",
    "        my_dictionary_HR_bps[person] = pandas.DataFrame({'Timestamp': my_dictionary_HR_mV[person].Timestamp, 'Value': 0})\n",
    "        my_dictionary_HR_nW[person] = pandas.DataFrame({'Timestamp': my_dictionary_HR_mV[person].Timestamp, 'Value': 0})\n",
    "        \n",
    "    elif ('Empatica' in  list_subDirectories) and ('eHealth' in list_subDirectories): \n",
    "        csv_files = find_csv_filenames(path + 'Empatica\\\\')\n",
    "        csv_file_BVP = find_list_string_startsWith(csv_files, 'BVP')\n",
    "        my_dictionary_HR_nW[person] = pandas.read_csv(path + 'Empatica\\\\'  + csv_file_BVP[0], sep=\";\", names=[\"Timestamp\", \"Value\"])\n",
    "        my_dictionary_HR_mV[person] = pandas.DataFrame({'Timestamp': my_dictionary_HR_nW[person].Timestamp, 'Value': 0})\n",
    "        my_dictionary_HR_bps[person] = pandas.DataFrame({'Timestamp': my_dictionary_HR_nW[person].Timestamp, 'Value': 0})\n",
    "    \n",
    "    else:\n",
    "        subdirectory = list_subDirectories[0]\n",
    "        csv_files = find_csv_filenames(path + subdirectory + '\\\\')\n",
    "        if (subdirectory == 'Empatica'):\n",
    "            csv_file_BVP = find_list_string_startsWith(csv_files, 'BVP')\n",
    "            my_dictionary_HR_nW[person] = pandas.read_csv(path + subdirectory + '\\\\'  + csv_file_BVP[0], sep=\";\", names=[\"Timestamp\", \"Value\"])\n",
    "            my_dictionary_HR_mV[person] = pandas.DataFrame({'Timestamp': my_dictionary_HR_nW[person].Timestamp, 'Value': 0})\n",
    "            my_dictionary_HR_bps[person] = pandas.DataFrame({'Timestamp': my_dictionary_HR_nW[person].Timestamp, 'Value': 0})\n",
    "\n",
    "        else:\n",
    "            csv_file_BPM = find_list_string_startsWith(csv_files, 'BPM')\n",
    "            my_dictionary_HR_bps[person] = pandas.read_csv(path + subdirectory + '\\\\'  + csv_file_BPM[0], sep=\",\", names=[\"Timestamp\", \"Value\"], skiprows=1)\n",
    "            my_dictionary_HR_mV[person] = pandas.DataFrame({'Timestamp': my_dictionary_HR_bps[person].Timestamp, 'Value': 0})\n",
    "            my_dictionary_HR_nW[person] = pandas.DataFrame({'Timestamp': my_dictionary_HR_bps[person].Timestamp, 'Value': 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding all necessary dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for research_participant in list_all_participants:\n",
    "    path = path_to_all_participants + research_participant + '\\\\'\n",
    "    csv_file = find_csv_filenames(path)\n",
    "    my_dictionary_images_timestamp[research_participant] = pandas.read_csv(path + csv_file[0], sep=\",\", names=[\"Image\", \"Timestamp\"], skiprows=6)\n",
    "    \n",
    "for research_participant in list_all_participants:\n",
    "    path = path_to_all_participants + research_participant + '\\\\'\n",
    "    fill_HR(research_participant, path)\n",
    "    my_dictionary_GSR[research_participant] = find_GSR(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing and converting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define converting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Provied by example\n",
    "def convert_timestamp_dictionary(my_dictionary):\n",
    "    for key, value in my_dictionary.items():\n",
    "        if (any(e < 10 ** 11 for e in value.Timestamp)):\n",
    "            my_dictionary[key].Timestamp = my_dictionary[key].Timestamp.apply(func=lambda t: 1000.0 * (t + 60*60))\n",
    "        else:\n",
    "            my_dictionary[key].Timestamp = my_dictionary[key].Timestamp.apply(func=lambda t: t + 60*60*1000)\n",
    "    return my_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process dictionary HR and images_timestamp which main purpose is converting timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in my_dictionary_images_timestamp.items():\n",
    "    my_dictionary_images_timestamp[key].Image = my_dictionary_images_timestamp[key].Image.apply(func=clear_filename)\n",
    "    my_dictionary_images_timestamp[key].Timestamp = my_dictionary_images_timestamp[key].Timestamp.apply(func=convert_psychopy_timestamp) \n",
    "\n",
    "my_dictionary_HR_mV = convert_timestamp_dictionary(my_dictionary_HR_mV)\n",
    "my_dictionary_HR_nW = convert_timestamp_dictionary(my_dictionary_HR_nW)\n",
    "my_dictionary_HR_bps = convert_timestamp_dictionary(my_dictionary_HR_bps)\n",
    "my_dictionary_GSR = convert_timestamp_dictionary(my_dictionary_GSR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing and converting images in Naps to dict with id = image name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dictionay_images_params = images_params.set_index('ID').T.to_dict('list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>Timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Landscapes_097_v</td>\n",
       "      <td>1.520850e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Landscapes_049_h</td>\n",
       "      <td>1.520850e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Objects_160_h</td>\n",
       "      <td>1.520850e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Animals_056_h</td>\n",
       "      <td>1.520850e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>People_038_h</td>\n",
       "      <td>1.520850e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>People_130_h</td>\n",
       "      <td>1.520850e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Landscapes_050_h</td>\n",
       "      <td>1.520850e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Landscapes_104_h</td>\n",
       "      <td>1.520850e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Animals_020_h</td>\n",
       "      <td>1.520850e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Landscapes_067_h</td>\n",
       "      <td>1.520850e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Landscapes_103_v</td>\n",
       "      <td>1.520850e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>People_208_h</td>\n",
       "      <td>1.520850e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Faces_159_h</td>\n",
       "      <td>1.520850e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Animals_090_h</td>\n",
       "      <td>1.520850e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Faces_134_h</td>\n",
       "      <td>1.520850e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Faces_234_h</td>\n",
       "      <td>1.520850e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Landscapes_054_h</td>\n",
       "      <td>1.520850e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Animals_035_h</td>\n",
       "      <td>1.520850e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Objects_014_h</td>\n",
       "      <td>1.520850e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>People_028_h</td>\n",
       "      <td>1.520850e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Landscapes_120_v</td>\n",
       "      <td>1.520850e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Faces_149_v</td>\n",
       "      <td>1.520850e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>People_198_h</td>\n",
       "      <td>1.520850e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>People_211_v</td>\n",
       "      <td>1.520850e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Objects_046_h</td>\n",
       "      <td>1.520850e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Faces_008_h</td>\n",
       "      <td>1.520850e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Objects_186_v</td>\n",
       "      <td>1.520850e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Landscapes_123_h</td>\n",
       "      <td>1.520850e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Objects_001_h</td>\n",
       "      <td>1.520850e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Objects_146_h</td>\n",
       "      <td>1.520850e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>People_110_h</td>\n",
       "      <td>1.520850e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>People_201_v</td>\n",
       "      <td>1.520850e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>People_127_h</td>\n",
       "      <td>1.520850e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Landscapes_061_h</td>\n",
       "      <td>1.520850e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Faces_202_v</td>\n",
       "      <td>1.520850e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Objects_132_h</td>\n",
       "      <td>1.520850e+12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Image     Timestamp\n",
       "0   Landscapes_097_v  1.520850e+12\n",
       "1   Landscapes_049_h  1.520850e+12\n",
       "2      Objects_160_h  1.520850e+12\n",
       "3      Animals_056_h  1.520850e+12\n",
       "4       People_038_h  1.520850e+12\n",
       "5       People_130_h  1.520850e+12\n",
       "6   Landscapes_050_h  1.520850e+12\n",
       "7   Landscapes_104_h  1.520850e+12\n",
       "8      Animals_020_h  1.520850e+12\n",
       "9   Landscapes_067_h  1.520850e+12\n",
       "10  Landscapes_103_v  1.520850e+12\n",
       "11      People_208_h  1.520850e+12\n",
       "12       Faces_159_h  1.520850e+12\n",
       "13     Animals_090_h  1.520850e+12\n",
       "14       Faces_134_h  1.520850e+12\n",
       "15       Faces_234_h  1.520850e+12\n",
       "16  Landscapes_054_h  1.520850e+12\n",
       "17     Animals_035_h  1.520850e+12\n",
       "18     Objects_014_h  1.520850e+12\n",
       "19      People_028_h  1.520850e+12\n",
       "20  Landscapes_120_v  1.520850e+12\n",
       "21       Faces_149_v  1.520850e+12\n",
       "22      People_198_h  1.520850e+12\n",
       "23      People_211_v  1.520850e+12\n",
       "24     Objects_046_h  1.520850e+12\n",
       "25       Faces_008_h  1.520850e+12\n",
       "26     Objects_186_v  1.520850e+12\n",
       "27  Landscapes_123_h  1.520850e+12\n",
       "28     Objects_001_h  1.520850e+12\n",
       "29     Objects_146_h  1.520850e+12\n",
       "30      People_110_h  1.520850e+12\n",
       "31      People_201_v  1.520850e+12\n",
       "32      People_127_h  1.520850e+12\n",
       "33  Landscapes_061_h  1.520850e+12\n",
       "34       Faces_202_v  1.520850e+12\n",
       "35     Objects_132_h  1.520850e+12"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check dictionary_images_timestamp\n",
    "my_dictionary_images_timestamp['B303']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding dataset from raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding mean of a provided DataFrame as argument which Timestamp in range (start_time, end_time) \n",
    "def get_mean(dataframe, start_time, end_time):\n",
    "    selected_rows = dataframe.loc[(dataframe['Timestamp'] >= start_time) & (dataframe['Timestamp'] < end_time)]\n",
    "    list_value = []\n",
    "    for reading in selected_rows.itertuples():\n",
    "        list_value.append(reading.Value)\n",
    "    if not list_value:\n",
    "        return 0\n",
    "    else:\n",
    "        return numpy.mean(list_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#define data_set like a list\n",
    "data_set = list()\n",
    "#for each person in list of participants\n",
    "for person in list_all_participants:\n",
    "    #get data of image and timestamp starting this image\n",
    "    images_timestamp = my_dictionary_images_timestamp[person]\n",
    "    for image_index in range(len(images_timestamp.Image)):\n",
    "        #get image id, valence, arousal\n",
    "        image = images_timestamp.Image[image_index]\n",
    "        valence = my_dictionay_images_params[image][4]\n",
    "        arousal = my_dictionay_images_params[image][5]\n",
    "        #get start_time\n",
    "        start_time = images_timestamp.Timestamp[image_index]\n",
    "        #get end_tie\n",
    "        end_time = images_timestamp.Timestamp[image_index + 1] if image_index != len(images_timestamp.Image) - 1 else sys.float_info.max\n",
    "        \n",
    "        #calculating all means\n",
    "        mean_mV = get_mean(my_dictionary_HR_mV[person], start_time, end_time)\n",
    "        mean_nW = get_mean(my_dictionary_HR_nW[person], start_time, end_time)\n",
    "        mean_bps = get_mean(my_dictionary_HR_bps[person], start_time, end_time)\n",
    "        mean_GSR = get_mean(my_dictionary_GSR[person], start_time, end_time)\n",
    "        \n",
    "        #add data into data_set as a tuple with (Image_id, mean of mV, mean of bps, mean of GSR, valence, arousal)\n",
    "        data = (image, mean_mV, mean_nW, mean_bps, mean_GSR, valence, arousal)\n",
    "        data_set.append(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3492"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We have 97 people and for each people we have 42 - 6 trail images to watch\n",
    "#So in totally we have 97 * 36 = 3492\n",
    "len(data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write data to a file which can be usefull in the future\n",
    "with open('data.csv', 'w') as fp:\n",
    "    fp.write(\"Image, mV, nW, bps, GSR, valence, arousal\\n\")\n",
    "    fp.write('\\n'.join('%s,%s,%s,%s,%s,%s,%s' %x for x in data_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Divide raw data set into training set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We decided 8:2 for training:test because size of data_set is only 3500\n",
    "#It could be 7:3 for second model\n",
    "n_dataset = len(data_set)\n",
    "n_training = int (n_dataset / 100 * 80.0)\n",
    "n_test = n_dataset - n_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get raw training data, raw test data\n",
    "raw_training = data_set[: n_training]\n",
    "raw_test = data_set[n_training:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pass a list into array of numpy for easier processing\n",
    "np_array_training = numpy.asarray(raw_training)\n",
    "#delte image name, valence and arousal and convert type to float\n",
    "X_training = numpy.delete(np_array_training, [0, 5, 6], 1)\n",
    "X_training = X_training.astype(float)\n",
    "\n",
    "#hold only valence and convert type to uint8\n",
    "Y_training_valence = numpy.delete(np_array_training, [0, 1, 2, 3, 4, 6], 1)\n",
    "Y_training_valence = ndarray.flatten(Y_training_valence)\n",
    "Y_training_valence = Y_training_valence.astype(float)\n",
    "Y_training_valence = Y_training_valence.astype(int)\n",
    "\n",
    "#hold only arousal and convert type to uint8\n",
    "Y_training_arousal = numpy.delete(np_array_training, [0, 1, 2, 3, 4, 5], 1)\n",
    "Y_training_arousal = ndarray.flatten(Y_training_arousal)\n",
    "Y_training_arousal = Y_training_arousal.astype(float)\n",
    "Y_training_arousal = Y_training_arousal.astype(int)\n",
    "\n",
    "#similar to test set\n",
    "np_array_test = numpy.asarray(raw_test)\n",
    "X_test = numpy.delete(np_array_test, [0, 5, 6], 1)\n",
    "X_test = X_test.astype(float)\n",
    "\n",
    "Y_test_valence = numpy.delete(np_array_test, [0, 1, 2, 3, 4, 6], 1)\n",
    "Y_test_valence = ndarray.flatten(Y_test_valence)\n",
    "Y_test_valence = Y_test_valence.astype(float)\n",
    "Y_test_valence = Y_test_valence.astype(int)\n",
    "\n",
    "Y_test_arousal = numpy.delete(np_array_test, [0, 1, 2, 3, 4, 5], 1)\n",
    "Y_test_arousal = ndarray.flatten(Y_test_arousal)\n",
    "Y_test_arousal = Y_test_arousal.astype(float)\n",
    "Y_test_valence = Y_test_valence.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking shape of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2793, 4)\n",
      "(2793,)\n",
      "(2793,)\n",
      "(699, 4)\n",
      "(699,)\n",
      "(699,)\n"
     ]
    }
   ],
   "source": [
    "#We expected to have (2794, 4) for training and (699, 4) for testing\n",
    "print(X_training.shape)\n",
    "print(Y_training_valence.shape)\n",
    "print(Y_training_arousal.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test_valence.shape)\n",
    "print(Y_test_arousal.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conver to softmax layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For the purpose of neutron network. We always decided to get one hot on coding\n",
    "#For example we have level of valence = 2, the neutron network will see it as 0 0 1 0 0 0 0 0 0 0\n",
    "#We convert it by categorical in keras\n",
    "n_classes = 10\n",
    "Y_training_valence = keras.utils.to_categorical(Y_training_valence, n_classes)\n",
    "Y_training_arousal = keras.utils.to_categorical(Y_training_arousal, n_classes)\n",
    "Y_test_valence = keras.utils.to_categorical(Y_test_valence, n_classes)\n",
    "Y_test_arousal = keras.utils.to_categorical(Y_test_arousal, n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Design neutral network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Design neutral network architecture with only one Dense layer\n",
    "#Activation softmax means output \n",
    "model_valence = Sequential()\n",
    "model_valence.add(Dense(64, activation = 'sigmoid', input_shape = (4,)))\n",
    "model_valence.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model_arousal = Sequential()\n",
    "model_arousal.add(Dense(64, activation = 'sigmoid', input_shape = (4,)))\n",
    "model_arousal.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 64)                320       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 970\n",
      "Trainable params: 970\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#We summary the network architecture\n",
    "#Expectation for param: first dense layer: 64*4+64(input) and second dense layer: 64*10+10(output)\n",
    "model_valence.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 64)                320       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 970\n",
      "Trainable params: 970\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#We summary the network architecture\n",
    "#Expectation for param: first dense layer: 64*4+64(input) and second dense layer: 64*10+10(output)\n",
    "model_arousal.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuration model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we use loss = mean squared error\n",
    "#gradient descent with learning rate= 0.01\n",
    "#optimize with accuracy\n",
    "model_valence.compile(loss='mean_squared_error', optimizer=SGD(lr=0.01), metrics=['accuracy'])\n",
    "model_arousal.compile(loss='mean_squared_error', optimizer=SGD(lr=0.01), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2793 samples, validate on 699 samples\n",
      "Epoch 1/100\n",
      "2793/2793 [==============================] - 0s 169us/step - loss: 0.0988 - acc: 0.1149 - val_loss: 0.0960 - val_acc: 0.1245\n",
      "Epoch 2/100\n",
      "2793/2793 [==============================] - 0s 45us/step - loss: 0.0972 - acc: 0.1174 - val_loss: 0.0946 - val_acc: 0.1302\n",
      "Epoch 3/100\n",
      "2793/2793 [==============================] - 0s 47us/step - loss: 0.0959 - acc: 0.1174 - val_loss: 0.0936 - val_acc: 0.1330\n",
      "Epoch 4/100\n",
      "2793/2793 [==============================] - 0s 40us/step - loss: 0.0948 - acc: 0.1185 - val_loss: 0.0927 - val_acc: 0.1330\n",
      "Epoch 5/100\n",
      "2793/2793 [==============================] - 0s 40us/step - loss: 0.0939 - acc: 0.1189 - val_loss: 0.0919 - val_acc: 0.1316\n",
      "Epoch 6/100\n",
      "2793/2793 [==============================] - 0s 44us/step - loss: 0.0931 - acc: 0.1214 - val_loss: 0.0912 - val_acc: 0.1345\n",
      "Epoch 7/100\n",
      "2793/2793 [==============================] - 0s 58us/step - loss: 0.0923 - acc: 0.1271 - val_loss: 0.0905 - val_acc: 0.1330\n",
      "Epoch 8/100\n",
      "2793/2793 [==============================] - 0s 34us/step - loss: 0.0917 - acc: 0.1275 - val_loss: 0.0900 - val_acc: 0.1545\n",
      "Epoch 9/100\n",
      "2793/2793 [==============================] - 0s 48us/step - loss: 0.0911 - acc: 0.1325 - val_loss: 0.0895 - val_acc: 0.1660\n",
      "Epoch 10/100\n",
      "2793/2793 [==============================] - 0s 40us/step - loss: 0.0905 - acc: 0.1364 - val_loss: 0.0891 - val_acc: 0.1717\n",
      "Epoch 11/100\n",
      "2793/2793 [==============================] - 0s 58us/step - loss: 0.0900 - acc: 0.1371 - val_loss: 0.0887 - val_acc: 0.1717\n",
      "Epoch 12/100\n",
      "2793/2793 [==============================] - 0s 61us/step - loss: 0.0896 - acc: 0.1378 - val_loss: 0.0884 - val_acc: 0.1660\n",
      "Epoch 13/100\n",
      "2793/2793 [==============================] - 0s 42us/step - loss: 0.0892 - acc: 0.1378 - val_loss: 0.0880 - val_acc: 0.1731\n",
      "Epoch 14/100\n",
      "2793/2793 [==============================] - 0s 55us/step - loss: 0.0888 - acc: 0.1389 - val_loss: 0.0878 - val_acc: 0.1731\n",
      "Epoch 15/100\n",
      "2793/2793 [==============================] - 0s 44us/step - loss: 0.0884 - acc: 0.1414 - val_loss: 0.0875 - val_acc: 0.1731\n",
      "Epoch 16/100\n",
      "2793/2793 [==============================] - 0s 51us/step - loss: 0.0880 - acc: 0.1404 - val_loss: 0.0872 - val_acc: 0.1745\n",
      "Epoch 17/100\n",
      "2793/2793 [==============================] - 0s 44us/step - loss: 0.0877 - acc: 0.1400 - val_loss: 0.0869 - val_acc: 0.1731\n",
      "Epoch 18/100\n",
      "2793/2793 [==============================] - 0s 52us/step - loss: 0.0873 - acc: 0.1400 - val_loss: 0.0867 - val_acc: 0.1745\n",
      "Epoch 19/100\n",
      "2793/2793 [==============================] - 0s 42us/step - loss: 0.0870 - acc: 0.1439 - val_loss: 0.0864 - val_acc: 0.1817\n",
      "Epoch 20/100\n",
      "2793/2793 [==============================] - 0s 43us/step - loss: 0.0867 - acc: 0.1801 - val_loss: 0.0862 - val_acc: 0.1803\n",
      "Epoch 21/100\n",
      "2793/2793 [==============================] - 0s 40us/step - loss: 0.0864 - acc: 0.1844 - val_loss: 0.0860 - val_acc: 0.1803\n",
      "Epoch 22/100\n",
      "2793/2793 [==============================] - 0s 50us/step - loss: 0.0861 - acc: 0.1901 - val_loss: 0.0858 - val_acc: 0.1874\n",
      "Epoch 23/100\n",
      "2793/2793 [==============================] - 0s 39us/step - loss: 0.0858 - acc: 0.2019 - val_loss: 0.0855 - val_acc: 0.1888\n",
      "Epoch 24/100\n",
      "2793/2793 [==============================] - 0s 50us/step - loss: 0.0855 - acc: 0.2001 - val_loss: 0.0853 - val_acc: 0.1917\n",
      "Epoch 25/100\n",
      "2793/2793 [==============================] - 0s 39us/step - loss: 0.0852 - acc: 0.1998 - val_loss: 0.0851 - val_acc: 0.1888\n",
      "Epoch 26/100\n",
      "2793/2793 [==============================] - 0s 50us/step - loss: 0.0850 - acc: 0.1998 - val_loss: 0.0849 - val_acc: 0.1888\n",
      "Epoch 27/100\n",
      "2793/2793 [==============================] - 0s 36us/step - loss: 0.0847 - acc: 0.1991 - val_loss: 0.0846 - val_acc: 0.1860\n",
      "Epoch 28/100\n",
      "2793/2793 [==============================] - 0s 32us/step - loss: 0.0845 - acc: 0.2005 - val_loss: 0.0844 - val_acc: 0.1888\n",
      "Epoch 29/100\n",
      "2793/2793 [==============================] - 0s 36us/step - loss: 0.0843 - acc: 0.2069 - val_loss: 0.0843 - val_acc: 0.1974\n",
      "Epoch 30/100\n",
      "2793/2793 [==============================] - 0s 32us/step - loss: 0.0842 - acc: 0.2302 - val_loss: 0.0842 - val_acc: 0.2003\n",
      "Epoch 31/100\n",
      "2793/2793 [==============================] - 0s 40us/step - loss: 0.0840 - acc: 0.2302 - val_loss: 0.0840 - val_acc: 0.2017\n",
      "Epoch 32/100\n",
      "2793/2793 [==============================] - 0s 39us/step - loss: 0.0839 - acc: 0.2306 - val_loss: 0.0840 - val_acc: 0.2074\n",
      "Epoch 33/100\n",
      "2793/2793 [==============================] - 0s 38us/step - loss: 0.0838 - acc: 0.2288 - val_loss: 0.0839 - val_acc: 0.2046\n",
      "Epoch 34/100\n",
      "2793/2793 [==============================] - 0s 47us/step - loss: 0.0837 - acc: 0.2334 - val_loss: 0.0838 - val_acc: 0.2175\n",
      "Epoch 35/100\n",
      "2793/2793 [==============================] - 0s 38us/step - loss: 0.0836 - acc: 0.2381 - val_loss: 0.0838 - val_acc: 0.2175\n",
      "Epoch 36/100\n",
      "2793/2793 [==============================] - 0s 41us/step - loss: 0.0836 - acc: 0.2392 - val_loss: 0.0837 - val_acc: 0.2246\n",
      "Epoch 37/100\n",
      "2793/2793 [==============================] - 0s 37us/step - loss: 0.0835 - acc: 0.2410 - val_loss: 0.0837 - val_acc: 0.2232\n",
      "Epoch 38/100\n",
      "2793/2793 [==============================] - 0s 50us/step - loss: 0.0835 - acc: 0.2392 - val_loss: 0.0837 - val_acc: 0.2203\n",
      "Epoch 39/100\n",
      "2793/2793 [==============================] - 0s 42us/step - loss: 0.0834 - acc: 0.2392 - val_loss: 0.0836 - val_acc: 0.2146\n",
      "Epoch 40/100\n",
      "2793/2793 [==============================] - 0s 42us/step - loss: 0.0834 - acc: 0.2399 - val_loss: 0.0836 - val_acc: 0.2175\n",
      "Epoch 41/100\n",
      "2793/2793 [==============================] - 0s 42us/step - loss: 0.0834 - acc: 0.2431 - val_loss: 0.0836 - val_acc: 0.2175\n",
      "Epoch 42/100\n",
      "2793/2793 [==============================] - 0s 45us/step - loss: 0.0833 - acc: 0.2470 - val_loss: 0.0836 - val_acc: 0.2260\n",
      "Epoch 43/100\n",
      "2793/2793 [==============================] - 0s 34us/step - loss: 0.0833 - acc: 0.2438 - val_loss: 0.0836 - val_acc: 0.2246\n",
      "Epoch 44/100\n",
      "2793/2793 [==============================] - 0s 39us/step - loss: 0.0833 - acc: 0.2474 - val_loss: 0.0835 - val_acc: 0.2275\n",
      "Epoch 45/100\n",
      "2793/2793 [==============================] - 0s 44us/step - loss: 0.0833 - acc: 0.2463 - val_loss: 0.0835 - val_acc: 0.2246\n",
      "Epoch 46/100\n",
      "2793/2793 [==============================] - 0s 32us/step - loss: 0.0832 - acc: 0.2470 - val_loss: 0.0835 - val_acc: 0.2260\n",
      "Epoch 47/100\n",
      "2793/2793 [==============================] - 0s 39us/step - loss: 0.0832 - acc: 0.2460 - val_loss: 0.0835 - val_acc: 0.2289\n",
      "Epoch 48/100\n",
      "2793/2793 [==============================] - 0s 41us/step - loss: 0.0832 - acc: 0.2467 - val_loss: 0.0835 - val_acc: 0.2260\n",
      "Epoch 49/100\n",
      "2793/2793 [==============================] - 0s 47us/step - loss: 0.0832 - acc: 0.2460 - val_loss: 0.0834 - val_acc: 0.2260\n",
      "Epoch 50/100\n",
      "2793/2793 [==============================] - 0s 32us/step - loss: 0.0831 - acc: 0.2470 - val_loss: 0.0834 - val_acc: 0.2260\n",
      "Epoch 51/100\n",
      "2793/2793 [==============================] - 0s 34us/step - loss: 0.0831 - acc: 0.2481 - val_loss: 0.0834 - val_acc: 0.2289\n",
      "Epoch 52/100\n",
      "2793/2793 [==============================] - 0s 47us/step - loss: 0.0830 - acc: 0.2478 - val_loss: 0.0834 - val_acc: 0.2289\n",
      "Epoch 53/100\n",
      "2793/2793 [==============================] - 0s 36us/step - loss: 0.0830 - acc: 0.2478 - val_loss: 0.0834 - val_acc: 0.2289\n",
      "Epoch 54/100\n",
      "2793/2793 [==============================] - 0s 41us/step - loss: 0.0830 - acc: 0.2481 - val_loss: 0.0833 - val_acc: 0.2303\n",
      "Epoch 55/100\n",
      "2793/2793 [==============================] - 0s 32us/step - loss: 0.0830 - acc: 0.2478 - val_loss: 0.0833 - val_acc: 0.2303\n",
      "Epoch 56/100\n",
      "2793/2793 [==============================] - 0s 36us/step - loss: 0.0830 - acc: 0.2478 - val_loss: 0.0833 - val_acc: 0.2275\n",
      "Epoch 57/100\n",
      "2793/2793 [==============================] - 0s 36us/step - loss: 0.0830 - acc: 0.2485 - val_loss: 0.0833 - val_acc: 0.2289\n",
      "Epoch 58/100\n",
      "2793/2793 [==============================] - 0s 42us/step - loss: 0.0830 - acc: 0.2470 - val_loss: 0.0833 - val_acc: 0.2289\n",
      "Epoch 59/100\n",
      "2793/2793 [==============================] - 0s 46us/step - loss: 0.0829 - acc: 0.2488 - val_loss: 0.0833 - val_acc: 0.2275\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2793/2793 [==============================] - 0s 40us/step - loss: 0.0829 - acc: 0.2470 - val_loss: 0.0833 - val_acc: 0.2260\n",
      "Epoch 61/100\n",
      "2793/2793 [==============================] - 0s 38us/step - loss: 0.0829 - acc: 0.2470 - val_loss: 0.0833 - val_acc: 0.2260\n",
      "Epoch 62/100\n",
      "2793/2793 [==============================] - 0s 34us/step - loss: 0.0829 - acc: 0.2470 - val_loss: 0.0833 - val_acc: 0.2260\n",
      "Epoch 63/100\n",
      "2793/2793 [==============================] - 0s 44us/step - loss: 0.0829 - acc: 0.2463 - val_loss: 0.0833 - val_acc: 0.2289\n",
      "Epoch 64/100\n",
      "2793/2793 [==============================] - 0s 31us/step - loss: 0.0829 - acc: 0.2470 - val_loss: 0.0833 - val_acc: 0.2275\n",
      "Epoch 65/100\n",
      "2793/2793 [==============================] - 0s 34us/step - loss: 0.0829 - acc: 0.2460 - val_loss: 0.0833 - val_acc: 0.2260\n",
      "Epoch 66/100\n",
      "2793/2793 [==============================] - 0s 41us/step - loss: 0.0829 - acc: 0.2478 - val_loss: 0.0833 - val_acc: 0.2289\n",
      "Epoch 67/100\n",
      "2793/2793 [==============================] - 0s 43us/step - loss: 0.0829 - acc: 0.2463 - val_loss: 0.0833 - val_acc: 0.2275\n",
      "Epoch 68/100\n",
      "2793/2793 [==============================] - 0s 55us/step - loss: 0.0829 - acc: 0.2470 - val_loss: 0.0833 - val_acc: 0.2289\n",
      "Epoch 69/100\n",
      "2793/2793 [==============================] - 0s 48us/step - loss: 0.0829 - acc: 0.2460 - val_loss: 0.0833 - val_acc: 0.2289\n",
      "Epoch 70/100\n",
      "2793/2793 [==============================] - 0s 44us/step - loss: 0.0829 - acc: 0.2463 - val_loss: 0.0832 - val_acc: 0.2260\n",
      "Epoch 71/100\n",
      "2793/2793 [==============================] - 0s 41us/step - loss: 0.0828 - acc: 0.2470 - val_loss: 0.0833 - val_acc: 0.2246\n",
      "Epoch 72/100\n",
      "2793/2793 [==============================] - 0s 45us/step - loss: 0.0828 - acc: 0.2456 - val_loss: 0.0832 - val_acc: 0.2246\n",
      "Epoch 73/100\n",
      "2793/2793 [==============================] - 0s 35us/step - loss: 0.0828 - acc: 0.2460 - val_loss: 0.0832 - val_acc: 0.2260\n",
      "Epoch 74/100\n",
      "2793/2793 [==============================] - 0s 38us/step - loss: 0.0828 - acc: 0.2470 - val_loss: 0.0832 - val_acc: 0.2318\n",
      "Epoch 75/100\n",
      "2793/2793 [==============================] - 0s 36us/step - loss: 0.0828 - acc: 0.2467 - val_loss: 0.0832 - val_acc: 0.2303\n",
      "Epoch 76/100\n",
      "2793/2793 [==============================] - 0s 51us/step - loss: 0.0828 - acc: 0.2453 - val_loss: 0.0832 - val_acc: 0.2260\n",
      "Epoch 77/100\n",
      "2793/2793 [==============================] - 0s 41us/step - loss: 0.0828 - acc: 0.2470 - val_loss: 0.0832 - val_acc: 0.2275\n",
      "Epoch 78/100\n",
      "2793/2793 [==============================] - 0s 42us/step - loss: 0.0828 - acc: 0.2460 - val_loss: 0.0832 - val_acc: 0.2275\n",
      "Epoch 79/100\n",
      "2793/2793 [==============================] - 0s 51us/step - loss: 0.0828 - acc: 0.2460 - val_loss: 0.0832 - val_acc: 0.2289\n",
      "Epoch 80/100\n",
      "2793/2793 [==============================] - 0s 52us/step - loss: 0.0828 - acc: 0.2463 - val_loss: 0.0831 - val_acc: 0.2289\n",
      "Epoch 81/100\n",
      "2793/2793 [==============================] - 0s 44us/step - loss: 0.0828 - acc: 0.2474 - val_loss: 0.0831 - val_acc: 0.2303\n",
      "Epoch 82/100\n",
      "2793/2793 [==============================] - 0s 39us/step - loss: 0.0828 - acc: 0.2463 - val_loss: 0.0831 - val_acc: 0.2289\n",
      "Epoch 83/100\n",
      "2793/2793 [==============================] - 0s 51us/step - loss: 0.0828 - acc: 0.2474 - val_loss: 0.0831 - val_acc: 0.2289\n",
      "Epoch 84/100\n",
      "2793/2793 [==============================] - 0s 41us/step - loss: 0.0828 - acc: 0.2467 - val_loss: 0.0831 - val_acc: 0.2332\n",
      "Epoch 85/100\n",
      "2793/2793 [==============================] - 0s 47us/step - loss: 0.0828 - acc: 0.2460 - val_loss: 0.0831 - val_acc: 0.2332\n",
      "Epoch 86/100\n",
      "2793/2793 [==============================] - 0s 31us/step - loss: 0.0827 - acc: 0.2470 - val_loss: 0.0831 - val_acc: 0.2318\n",
      "Epoch 87/100\n",
      "2793/2793 [==============================] - 0s 38us/step - loss: 0.0827 - acc: 0.2460 - val_loss: 0.0830 - val_acc: 0.2303\n",
      "Epoch 88/100\n",
      "2793/2793 [==============================] - 0s 37us/step - loss: 0.0827 - acc: 0.2470 - val_loss: 0.0830 - val_acc: 0.2303\n",
      "Epoch 89/100\n",
      "2793/2793 [==============================] - 0s 48us/step - loss: 0.0827 - acc: 0.2470 - val_loss: 0.0830 - val_acc: 0.2332\n",
      "Epoch 90/100\n",
      "2793/2793 [==============================] - 0s 47us/step - loss: 0.0827 - acc: 0.2474 - val_loss: 0.0830 - val_acc: 0.2303\n",
      "Epoch 91/100\n",
      "2793/2793 [==============================] - 0s 56us/step - loss: 0.0827 - acc: 0.2467 - val_loss: 0.0830 - val_acc: 0.2332\n",
      "Epoch 92/100\n",
      "2793/2793 [==============================] - 0s 39us/step - loss: 0.0827 - acc: 0.2478 - val_loss: 0.0830 - val_acc: 0.2318\n",
      "Epoch 93/100\n",
      "2793/2793 [==============================] - 0s 32us/step - loss: 0.0827 - acc: 0.2463 - val_loss: 0.0830 - val_acc: 0.2332\n",
      "Epoch 94/100\n",
      "2793/2793 [==============================] - 0s 33us/step - loss: 0.0827 - acc: 0.2470 - val_loss: 0.0830 - val_acc: 0.2289\n",
      "Epoch 95/100\n",
      "2793/2793 [==============================] - 0s 37us/step - loss: 0.0827 - acc: 0.2470 - val_loss: 0.0829 - val_acc: 0.2332\n",
      "Epoch 96/100\n",
      "2793/2793 [==============================] - 0s 44us/step - loss: 0.0827 - acc: 0.2470 - val_loss: 0.0829 - val_acc: 0.2332\n",
      "Epoch 97/100\n",
      "2793/2793 [==============================] - 0s 37us/step - loss: 0.0827 - acc: 0.2463 - val_loss: 0.0829 - val_acc: 0.2346\n",
      "Epoch 98/100\n",
      "2793/2793 [==============================] - 0s 42us/step - loss: 0.0827 - acc: 0.2467 - val_loss: 0.0829 - val_acc: 0.2332\n",
      "Epoch 99/100\n",
      "2793/2793 [==============================] - 0s 36us/step - loss: 0.0827 - acc: 0.2470 - val_loss: 0.0829 - val_acc: 0.2332\n",
      "Epoch 100/100\n",
      "2793/2793 [==============================] - 0s 38us/step - loss: 0.0827 - acc: 0.2470 - val_loss: 0.0829 - val_acc: 0.2332\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e479819668>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train model with training set, 100 epochs(100 times), verbose = 1 means we will see result after one epochs\n",
    "#validation by test_set\n",
    "model_valence.fit(X_training, Y_training_valence, epochs = 100, verbose = 1, validation_data=(X_test, Y_test_valence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2793 samples, validate on 699 samples\n",
      "Epoch 1/100\n",
      "2793/2793 [==============================] - 0s 73us/step - loss: 0.0949 - acc: 0.0616 - val_loss: 0.0951 - val_acc: 0.1102\n",
      "Epoch 2/100\n",
      "2793/2793 [==============================] - 0s 39us/step - loss: 0.0937 - acc: 0.1214 - val_loss: 0.0943 - val_acc: 0.1359\n",
      "Epoch 3/100\n",
      "2793/2793 [==============================] - 0s 47us/step - loss: 0.0929 - acc: 0.1389 - val_loss: 0.0937 - val_acc: 0.1373\n",
      "Epoch 4/100\n",
      "2793/2793 [==============================] - 0s 43us/step - loss: 0.0922 - acc: 0.1686 - val_loss: 0.0931 - val_acc: 0.1602\n",
      "Epoch 5/100\n",
      "2793/2793 [==============================] - 0s 52us/step - loss: 0.0916 - acc: 0.1969 - val_loss: 0.0925 - val_acc: 0.1674\n",
      "Epoch 6/100\n",
      "2793/2793 [==============================] - 0s 50us/step - loss: 0.0910 - acc: 0.2019 - val_loss: 0.0920 - val_acc: 0.1688\n",
      "Epoch 7/100\n",
      "2793/2793 [==============================] - 0s 50us/step - loss: 0.0904 - acc: 0.2026 - val_loss: 0.0915 - val_acc: 0.1731\n",
      "Epoch 8/100\n",
      "2793/2793 [==============================] - 0s 52us/step - loss: 0.0899 - acc: 0.2023 - val_loss: 0.0910 - val_acc: 0.1731\n",
      "Epoch 9/100\n",
      "2793/2793 [==============================] - 0s 39us/step - loss: 0.0895 - acc: 0.2034 - val_loss: 0.0905 - val_acc: 0.1745\n",
      "Epoch 10/100\n",
      "2793/2793 [==============================] - 0s 42us/step - loss: 0.0891 - acc: 0.2059 - val_loss: 0.0901 - val_acc: 0.1788\n",
      "Epoch 11/100\n",
      "2793/2793 [==============================] - 0s 42us/step - loss: 0.0887 - acc: 0.2102 - val_loss: 0.0897 - val_acc: 0.1788\n",
      "Epoch 12/100\n",
      "2793/2793 [==============================] - 0s 42us/step - loss: 0.0883 - acc: 0.2109 - val_loss: 0.0894 - val_acc: 0.1817\n",
      "Epoch 13/100\n",
      "2793/2793 [==============================] - 0s 38us/step - loss: 0.0880 - acc: 0.2130 - val_loss: 0.0890 - val_acc: 0.1845\n",
      "Epoch 14/100\n",
      "2793/2793 [==============================] - 0s 40us/step - loss: 0.0877 - acc: 0.2216 - val_loss: 0.0887 - val_acc: 0.1903\n",
      "Epoch 15/100\n",
      "2793/2793 [==============================] - 0s 46us/step - loss: 0.0874 - acc: 0.2245 - val_loss: 0.0884 - val_acc: 0.1917\n",
      "Epoch 16/100\n",
      "2793/2793 [==============================] - 0s 39us/step - loss: 0.0871 - acc: 0.2295 - val_loss: 0.0881 - val_acc: 0.2017\n",
      "Epoch 17/100\n",
      "2793/2793 [==============================] - 0s 48us/step - loss: 0.0868 - acc: 0.2299 - val_loss: 0.0879 - val_acc: 0.2003\n",
      "Epoch 18/100\n",
      "2793/2793 [==============================] - 0s 37us/step - loss: 0.0865 - acc: 0.2291 - val_loss: 0.0876 - val_acc: 0.2003\n",
      "Epoch 19/100\n",
      "2793/2793 [==============================] - 0s 47us/step - loss: 0.0862 - acc: 0.2299 - val_loss: 0.0873 - val_acc: 0.2003\n",
      "Epoch 20/100\n",
      "2793/2793 [==============================] - 0s 46us/step - loss: 0.0860 - acc: 0.2306 - val_loss: 0.0871 - val_acc: 0.2017\n",
      "Epoch 21/100\n",
      "2793/2793 [==============================] - 0s 40us/step - loss: 0.0857 - acc: 0.2317 - val_loss: 0.0868 - val_acc: 0.2046\n",
      "Epoch 22/100\n",
      "2793/2793 [==============================] - 0s 45us/step - loss: 0.0854 - acc: 0.2331 - val_loss: 0.0865 - val_acc: 0.2060\n",
      "Epoch 23/100\n",
      "2793/2793 [==============================] - 0s 43us/step - loss: 0.0851 - acc: 0.2327 - val_loss: 0.0862 - val_acc: 0.2060\n",
      "Epoch 24/100\n",
      "2793/2793 [==============================] - 0s 46us/step - loss: 0.0849 - acc: 0.2349 - val_loss: 0.0859 - val_acc: 0.2060\n",
      "Epoch 25/100\n",
      "2793/2793 [==============================] - 0s 42us/step - loss: 0.0847 - acc: 0.2363 - val_loss: 0.0856 - val_acc: 0.2074\n",
      "Epoch 26/100\n",
      "2793/2793 [==============================] - 0s 45us/step - loss: 0.0844 - acc: 0.2352 - val_loss: 0.0853 - val_acc: 0.2074\n",
      "Epoch 27/100\n",
      "2793/2793 [==============================] - 0s 47us/step - loss: 0.0842 - acc: 0.2367 - val_loss: 0.0849 - val_acc: 0.2132\n",
      "Epoch 28/100\n",
      "2793/2793 [==============================] - 0s 48us/step - loss: 0.0839 - acc: 0.2374 - val_loss: 0.0847 - val_acc: 0.2160\n",
      "Epoch 29/100\n",
      "2793/2793 [==============================] - 0s 48us/step - loss: 0.0836 - acc: 0.2385 - val_loss: 0.0845 - val_acc: 0.2217\n",
      "Epoch 30/100\n",
      "2793/2793 [==============================] - 0s 42us/step - loss: 0.0835 - acc: 0.2438 - val_loss: 0.0844 - val_acc: 0.2389\n",
      "Epoch 31/100\n",
      "2793/2793 [==============================] - 0s 68us/step - loss: 0.0833 - acc: 0.2435 - val_loss: 0.0842 - val_acc: 0.2375\n",
      "Epoch 32/100\n",
      "2793/2793 [==============================] - 0s 56us/step - loss: 0.0832 - acc: 0.2624 - val_loss: 0.0840 - val_acc: 0.2403\n",
      "Epoch 33/100\n",
      "2793/2793 [==============================] - 0s 44us/step - loss: 0.0831 - acc: 0.2624 - val_loss: 0.0839 - val_acc: 0.2461\n",
      "Epoch 34/100\n",
      "2793/2793 [==============================] - 0s 31us/step - loss: 0.0830 - acc: 0.2628 - val_loss: 0.0838 - val_acc: 0.2518\n",
      "Epoch 35/100\n",
      "2793/2793 [==============================] - 0s 51us/step - loss: 0.0828 - acc: 0.2628 - val_loss: 0.0836 - val_acc: 0.2475\n",
      "Epoch 36/100\n",
      "2793/2793 [==============================] - 0s 67us/step - loss: 0.0827 - acc: 0.2635 - val_loss: 0.0835 - val_acc: 0.2518\n",
      "Epoch 37/100\n",
      "2793/2793 [==============================] - 0s 53us/step - loss: 0.0826 - acc: 0.2632 - val_loss: 0.0834 - val_acc: 0.2489\n",
      "Epoch 38/100\n",
      "2793/2793 [==============================] - 0s 53us/step - loss: 0.0825 - acc: 0.2639 - val_loss: 0.0832 - val_acc: 0.2475\n",
      "Epoch 39/100\n",
      "2793/2793 [==============================] - 0s 65us/step - loss: 0.0824 - acc: 0.2642 - val_loss: 0.0831 - val_acc: 0.2461\n",
      "Epoch 40/100\n",
      "2793/2793 [==============================] - 0s 51us/step - loss: 0.0824 - acc: 0.2635 - val_loss: 0.0830 - val_acc: 0.2461\n",
      "Epoch 41/100\n",
      "2793/2793 [==============================] - 0s 49us/step - loss: 0.0823 - acc: 0.2642 - val_loss: 0.0829 - val_acc: 0.2504\n",
      "Epoch 42/100\n",
      "2793/2793 [==============================] - 0s 49us/step - loss: 0.0822 - acc: 0.2632 - val_loss: 0.0828 - val_acc: 0.2489\n",
      "Epoch 43/100\n",
      "2793/2793 [==============================] - 0s 42us/step - loss: 0.0822 - acc: 0.2628 - val_loss: 0.0827 - val_acc: 0.2504\n",
      "Epoch 44/100\n",
      "2793/2793 [==============================] - 0s 46us/step - loss: 0.0821 - acc: 0.2632 - val_loss: 0.0826 - val_acc: 0.2504\n",
      "Epoch 45/100\n",
      "2793/2793 [==============================] - 0s 46us/step - loss: 0.0820 - acc: 0.2642 - val_loss: 0.0825 - val_acc: 0.2489\n",
      "Epoch 46/100\n",
      "2793/2793 [==============================] - 0s 49us/step - loss: 0.0820 - acc: 0.2635 - val_loss: 0.0824 - val_acc: 0.2504\n",
      "Epoch 47/100\n",
      "2793/2793 [==============================] - 0s 47us/step - loss: 0.0819 - acc: 0.2635 - val_loss: 0.0823 - val_acc: 0.2518\n",
      "Epoch 48/100\n",
      "2793/2793 [==============================] - 0s 40us/step - loss: 0.0819 - acc: 0.2639 - val_loss: 0.0822 - val_acc: 0.2532\n",
      "Epoch 49/100\n",
      "2793/2793 [==============================] - 0s 42us/step - loss: 0.0818 - acc: 0.2632 - val_loss: 0.0822 - val_acc: 0.2532\n",
      "Epoch 50/100\n",
      "2793/2793 [==============================] - 0s 44us/step - loss: 0.0817 - acc: 0.2632 - val_loss: 0.0821 - val_acc: 0.2546\n",
      "Epoch 51/100\n",
      "2793/2793 [==============================] - 0s 44us/step - loss: 0.0817 - acc: 0.2599 - val_loss: 0.0820 - val_acc: 0.2532\n",
      "Epoch 52/100\n",
      "2793/2793 [==============================] - 0s 38us/step - loss: 0.0816 - acc: 0.2649 - val_loss: 0.0819 - val_acc: 0.2546\n",
      "Epoch 53/100\n",
      "2793/2793 [==============================] - 0s 46us/step - loss: 0.0816 - acc: 0.2671 - val_loss: 0.0819 - val_acc: 0.2618\n",
      "Epoch 54/100\n",
      "2793/2793 [==============================] - 0s 48us/step - loss: 0.0815 - acc: 0.2685 - val_loss: 0.0818 - val_acc: 0.2632\n",
      "Epoch 55/100\n",
      "2793/2793 [==============================] - 0s 47us/step - loss: 0.0815 - acc: 0.2692 - val_loss: 0.0817 - val_acc: 0.2647\n",
      "Epoch 56/100\n",
      "2793/2793 [==============================] - 0s 45us/step - loss: 0.0814 - acc: 0.2689 - val_loss: 0.0817 - val_acc: 0.2632\n",
      "Epoch 57/100\n",
      "2793/2793 [==============================] - 0s 43us/step - loss: 0.0814 - acc: 0.2703 - val_loss: 0.0816 - val_acc: 0.2647\n",
      "Epoch 58/100\n",
      "2793/2793 [==============================] - 0s 41us/step - loss: 0.0813 - acc: 0.2689 - val_loss: 0.0816 - val_acc: 0.2661\n",
      "Epoch 59/100\n",
      "2793/2793 [==============================] - 0s 46us/step - loss: 0.0813 - acc: 0.2700 - val_loss: 0.0816 - val_acc: 0.2661\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2793/2793 [==============================] - 0s 48us/step - loss: 0.0812 - acc: 0.2703 - val_loss: 0.0815 - val_acc: 0.2675\n",
      "Epoch 61/100\n",
      "2793/2793 [==============================] - 0s 35us/step - loss: 0.0812 - acc: 0.2692 - val_loss: 0.0815 - val_acc: 0.2704\n",
      "Epoch 62/100\n",
      "2793/2793 [==============================] - 0s 39us/step - loss: 0.0812 - acc: 0.2700 - val_loss: 0.0814 - val_acc: 0.2704\n",
      "Epoch 63/100\n",
      "2793/2793 [==============================] - 0s 33us/step - loss: 0.0811 - acc: 0.2700 - val_loss: 0.0814 - val_acc: 0.2704\n",
      "Epoch 64/100\n",
      "2793/2793 [==============================] - 0s 35us/step - loss: 0.0811 - acc: 0.2675 - val_loss: 0.0813 - val_acc: 0.2704\n",
      "Epoch 65/100\n",
      "2793/2793 [==============================] - 0s 30us/step - loss: 0.0811 - acc: 0.2689 - val_loss: 0.0813 - val_acc: 0.2690\n",
      "Epoch 66/100\n",
      "2793/2793 [==============================] - 0s 33us/step - loss: 0.0810 - acc: 0.2710 - val_loss: 0.0813 - val_acc: 0.2690\n",
      "Epoch 67/100\n",
      "2793/2793 [==============================] - 0s 37us/step - loss: 0.0810 - acc: 0.2682 - val_loss: 0.0812 - val_acc: 0.2690\n",
      "Epoch 68/100\n",
      "2793/2793 [==============================] - 0s 34us/step - loss: 0.0810 - acc: 0.2696 - val_loss: 0.0812 - val_acc: 0.2690\n",
      "Epoch 69/100\n",
      "2793/2793 [==============================] - 0s 31us/step - loss: 0.0809 - acc: 0.2707 - val_loss: 0.0812 - val_acc: 0.2704\n",
      "Epoch 70/100\n",
      "2793/2793 [==============================] - 0s 30us/step - loss: 0.0809 - acc: 0.2714 - val_loss: 0.0811 - val_acc: 0.2704\n",
      "Epoch 71/100\n",
      "2793/2793 [==============================] - 0s 35us/step - loss: 0.0809 - acc: 0.2710 - val_loss: 0.0811 - val_acc: 0.2690\n",
      "Epoch 72/100\n",
      "2793/2793 [==============================] - 0s 38us/step - loss: 0.0808 - acc: 0.2714 - val_loss: 0.0810 - val_acc: 0.2704\n",
      "Epoch 73/100\n",
      "2793/2793 [==============================] - 0s 32us/step - loss: 0.0808 - acc: 0.2735 - val_loss: 0.0810 - val_acc: 0.2675\n",
      "Epoch 74/100\n",
      "2793/2793 [==============================] - 0s 30us/step - loss: 0.0807 - acc: 0.2732 - val_loss: 0.0810 - val_acc: 0.2675\n",
      "Epoch 75/100\n",
      "2793/2793 [==============================] - 0s 29us/step - loss: 0.0807 - acc: 0.2725 - val_loss: 0.0809 - val_acc: 0.2675\n",
      "Epoch 76/100\n",
      "2793/2793 [==============================] - 0s 30us/step - loss: 0.0807 - acc: 0.2743 - val_loss: 0.0809 - val_acc: 0.2647\n",
      "Epoch 77/100\n",
      "2793/2793 [==============================] - 0s 40us/step - loss: 0.0807 - acc: 0.2732 - val_loss: 0.0809 - val_acc: 0.2632\n",
      "Epoch 78/100\n",
      "2793/2793 [==============================] - 0s 33us/step - loss: 0.0806 - acc: 0.2718 - val_loss: 0.0809 - val_acc: 0.2647\n",
      "Epoch 79/100\n",
      "2793/2793 [==============================] - 0s 29us/step - loss: 0.0806 - acc: 0.2728 - val_loss: 0.0808 - val_acc: 0.2675\n",
      "Epoch 80/100\n",
      "2793/2793 [==============================] - 0s 30us/step - loss: 0.0806 - acc: 0.2718 - val_loss: 0.0808 - val_acc: 0.2675\n",
      "Epoch 81/100\n",
      "2793/2793 [==============================] - 0s 29us/step - loss: 0.0806 - acc: 0.2703 - val_loss: 0.0808 - val_acc: 0.2675\n",
      "Epoch 82/100\n",
      "2793/2793 [==============================] - 0s 32us/step - loss: 0.0806 - acc: 0.2743 - val_loss: 0.0808 - val_acc: 0.2690\n",
      "Epoch 83/100\n",
      "2793/2793 [==============================] - 0s 37us/step - loss: 0.0805 - acc: 0.2696 - val_loss: 0.0808 - val_acc: 0.2675\n",
      "Epoch 84/100\n",
      "2793/2793 [==============================] - 0s 33us/step - loss: 0.0805 - acc: 0.2703 - val_loss: 0.0807 - val_acc: 0.2675\n",
      "Epoch 85/100\n",
      "2793/2793 [==============================] - 0s 29us/step - loss: 0.0805 - acc: 0.2703 - val_loss: 0.0807 - val_acc: 0.2675\n",
      "Epoch 86/100\n",
      "2793/2793 [==============================] - 0s 29us/step - loss: 0.0805 - acc: 0.2710 - val_loss: 0.0807 - val_acc: 0.2690\n",
      "Epoch 87/100\n",
      "2793/2793 [==============================] - 0s 31us/step - loss: 0.0805 - acc: 0.2710 - val_loss: 0.0807 - val_acc: 0.2675\n",
      "Epoch 88/100\n",
      "2793/2793 [==============================] - 0s 29us/step - loss: 0.0804 - acc: 0.2707 - val_loss: 0.0807 - val_acc: 0.2690\n",
      "Epoch 89/100\n",
      "2793/2793 [==============================] - 0s 33us/step - loss: 0.0804 - acc: 0.2721 - val_loss: 0.0807 - val_acc: 0.2704\n",
      "Epoch 90/100\n",
      "2793/2793 [==============================] - 0s 33us/step - loss: 0.0804 - acc: 0.2714 - val_loss: 0.0806 - val_acc: 0.2747\n",
      "Epoch 91/100\n",
      "2793/2793 [==============================] - 0s 33us/step - loss: 0.0804 - acc: 0.2714 - val_loss: 0.0806 - val_acc: 0.2718\n",
      "Epoch 92/100\n",
      "2793/2793 [==============================] - 0s 30us/step - loss: 0.0804 - acc: 0.2718 - val_loss: 0.0806 - val_acc: 0.2718\n",
      "Epoch 93/100\n",
      "2793/2793 [==============================] - 0s 31us/step - loss: 0.0804 - acc: 0.2725 - val_loss: 0.0806 - val_acc: 0.2718\n",
      "Epoch 94/100\n",
      "2793/2793 [==============================] - 0s 30us/step - loss: 0.0803 - acc: 0.2725 - val_loss: 0.0806 - val_acc: 0.2704\n",
      "Epoch 95/100\n",
      "2793/2793 [==============================] - 0s 31us/step - loss: 0.0803 - acc: 0.2714 - val_loss: 0.0806 - val_acc: 0.2704\n",
      "Epoch 96/100\n",
      "2793/2793 [==============================] - 0s 33us/step - loss: 0.0803 - acc: 0.2728 - val_loss: 0.0806 - val_acc: 0.2718\n",
      "Epoch 97/100\n",
      "2793/2793 [==============================] - 0s 33us/step - loss: 0.0803 - acc: 0.2725 - val_loss: 0.0806 - val_acc: 0.2704\n",
      "Epoch 98/100\n",
      "2793/2793 [==============================] - 0s 35us/step - loss: 0.0803 - acc: 0.2718 - val_loss: 0.0805 - val_acc: 0.2675\n",
      "Epoch 99/100\n",
      "2793/2793 [==============================] - 0s 32us/step - loss: 0.0803 - acc: 0.2725 - val_loss: 0.0805 - val_acc: 0.2675\n",
      "Epoch 100/100\n",
      "2793/2793 [==============================] - 0s 29us/step - loss: 0.0803 - acc: 0.2725 - val_loss: 0.0805 - val_acc: 0.2704\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e47981a470>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_arousal.fit(X_training, Y_training_arousal, epochs = 100, verbose = 1, validation_data=(X_test, Y_test_arousal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Design neutral network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We add more layer into network and using activation tanh\n",
    "model_valence = Sequential()\n",
    "model_valence.add(Dense(64, activation = 'tanh', input_shape = (4,)))\n",
    "model_valence.add(Dense(64, activation = 'tanh'))\n",
    "model_valence.add(Dense(64, activation = 'tanh'))\n",
    "model_valence.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model_arousal = Sequential()\n",
    "model_arousal.add(Dense(64, activation = 'tanh', input_shape = (4,)))\n",
    "model_arousal.add(Dense(64, activation = 'tanh'))\n",
    "model_arousal.add(Dense(64, activation = 'tanh'))\n",
    "model_arousal.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuration model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We ise lower learning rate and loss = categorical_crossentropy\n",
    "model_valence.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.001), metrics=['accuracy'])\n",
    "model_arousal.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.001), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2793 samples, validate on 699 samples\n",
      "Epoch 1/100\n",
      "2793/2793 [==============================] - 0s 141us/step - loss: 2.3439 - acc: 0.1507 - val_loss: 2.2195 - val_acc: 0.1617\n",
      "Epoch 2/100\n",
      "2793/2793 [==============================] - 0s 38us/step - loss: 2.1371 - acc: 0.2023 - val_loss: 2.0642 - val_acc: 0.2074\n",
      "Epoch 3/100\n",
      "2793/2793 [==============================] - 0s 46us/step - loss: 2.0397 - acc: 0.2324 - val_loss: 1.9942 - val_acc: 0.2217\n",
      "Epoch 4/100\n",
      "2793/2793 [==============================] - 0s 45us/step - loss: 1.9899 - acc: 0.2370 - val_loss: 1.9565 - val_acc: 0.2203\n",
      "Epoch 5/100\n",
      "2793/2793 [==============================] - 0s 43us/step - loss: 1.9606 - acc: 0.2356 - val_loss: 1.9345 - val_acc: 0.2203\n",
      "Epoch 6/100\n",
      "2793/2793 [==============================] - 0s 48us/step - loss: 1.9417 - acc: 0.2377 - val_loss: 1.9136 - val_acc: 0.2303\n",
      "Epoch 7/100\n",
      "2793/2793 [==============================] - 0s 49us/step - loss: 1.9264 - acc: 0.2385 - val_loss: 1.9010 - val_acc: 0.2289\n",
      "Epoch 8/100\n",
      "2793/2793 [==============================] - 0s 48us/step - loss: 1.9167 - acc: 0.2345 - val_loss: 1.8935 - val_acc: 0.2375\n",
      "Epoch 9/100\n",
      "2793/2793 [==============================] - 0s 40us/step - loss: 1.9067 - acc: 0.2399 - val_loss: 1.8870 - val_acc: 0.2289\n",
      "Epoch 10/100\n",
      "2793/2793 [==============================] - 0s 42us/step - loss: 1.9005 - acc: 0.2449 - val_loss: 1.8873 - val_acc: 0.2175\n",
      "Epoch 11/100\n",
      "2793/2793 [==============================] - 0s 51us/step - loss: 1.8954 - acc: 0.2427 - val_loss: 1.8812 - val_acc: 0.2132\n",
      "Epoch 12/100\n",
      "2793/2793 [==============================] - 0s 40us/step - loss: 1.8909 - acc: 0.2435 - val_loss: 1.8740 - val_acc: 0.2175\n",
      "Epoch 13/100\n",
      "2793/2793 [==============================] - 0s 49us/step - loss: 1.8862 - acc: 0.2427 - val_loss: 1.8686 - val_acc: 0.2389\n",
      "Epoch 14/100\n",
      "2793/2793 [==============================] - 0s 49us/step - loss: 1.8832 - acc: 0.2467 - val_loss: 1.8651 - val_acc: 0.2375\n",
      "Epoch 15/100\n",
      "2793/2793 [==============================] - 0s 40us/step - loss: 1.8804 - acc: 0.2478 - val_loss: 1.8622 - val_acc: 0.2432\n",
      "Epoch 16/100\n",
      "2793/2793 [==============================] - 0s 46us/step - loss: 1.8781 - acc: 0.2499 - val_loss: 1.8620 - val_acc: 0.2346\n",
      "Epoch 17/100\n",
      "2793/2793 [==============================] - 0s 44us/step - loss: 1.8755 - acc: 0.2492 - val_loss: 1.8577 - val_acc: 0.2361\n",
      "Epoch 18/100\n",
      "2793/2793 [==============================] - 0s 47us/step - loss: 1.8735 - acc: 0.2506 - val_loss: 1.8561 - val_acc: 0.2346\n",
      "Epoch 19/100\n",
      "2793/2793 [==============================] - 0s 40us/step - loss: 1.8711 - acc: 0.2506 - val_loss: 1.8536 - val_acc: 0.2375\n",
      "Epoch 20/100\n",
      "2793/2793 [==============================] - 0s 45us/step - loss: 1.8688 - acc: 0.2510 - val_loss: 1.8522 - val_acc: 0.2361\n",
      "Epoch 21/100\n",
      "2793/2793 [==============================] - 0s 55us/step - loss: 1.8675 - acc: 0.2499 - val_loss: 1.8515 - val_acc: 0.2375\n",
      "Epoch 22/100\n",
      "2793/2793 [==============================] - 0s 66us/step - loss: 1.8661 - acc: 0.2499 - val_loss: 1.8471 - val_acc: 0.2475\n",
      "Epoch 23/100\n",
      "2793/2793 [==============================] - 0s 50us/step - loss: 1.8649 - acc: 0.2506 - val_loss: 1.8446 - val_acc: 0.2489\n",
      "Epoch 24/100\n",
      "2793/2793 [==============================] - 0s 50us/step - loss: 1.8633 - acc: 0.2503 - val_loss: 1.8432 - val_acc: 0.2532\n",
      "Epoch 25/100\n",
      "2793/2793 [==============================] - 0s 43us/step - loss: 1.8628 - acc: 0.2510 - val_loss: 1.8432 - val_acc: 0.2446\n",
      "Epoch 26/100\n",
      "2793/2793 [==============================] - 0s 42us/step - loss: 1.8613 - acc: 0.2499 - val_loss: 1.8418 - val_acc: 0.2461\n",
      "Epoch 27/100\n",
      "2793/2793 [==============================] - 0s 48us/step - loss: 1.8607 - acc: 0.2492 - val_loss: 1.8402 - val_acc: 0.2518\n",
      "Epoch 28/100\n",
      "2793/2793 [==============================] - 0s 41us/step - loss: 1.8597 - acc: 0.2478 - val_loss: 1.8391 - val_acc: 0.2561\n",
      "Epoch 29/100\n",
      "2793/2793 [==============================] - 0s 37us/step - loss: 1.8593 - acc: 0.2481 - val_loss: 1.8365 - val_acc: 0.2504\n",
      "Epoch 30/100\n",
      "2793/2793 [==============================] - 0s 51us/step - loss: 1.8587 - acc: 0.2510 - val_loss: 1.8379 - val_acc: 0.2561\n",
      "Epoch 31/100\n",
      "2793/2793 [==============================] - 0s 42us/step - loss: 1.8574 - acc: 0.2524 - val_loss: 1.8370 - val_acc: 0.2446\n",
      "Epoch 32/100\n",
      "2793/2793 [==============================] - 0s 39us/step - loss: 1.8570 - acc: 0.2510 - val_loss: 1.8348 - val_acc: 0.2489\n",
      "Epoch 33/100\n",
      "2793/2793 [==============================] - 0s 42us/step - loss: 1.8559 - acc: 0.2496 - val_loss: 1.8336 - val_acc: 0.2504\n",
      "Epoch 34/100\n",
      "2793/2793 [==============================] - 0s 50us/step - loss: 1.8555 - acc: 0.2503 - val_loss: 1.8341 - val_acc: 0.2532\n",
      "Epoch 35/100\n",
      "2793/2793 [==============================] - 0s 41us/step - loss: 1.8548 - acc: 0.2492 - val_loss: 1.8326 - val_acc: 0.2532\n",
      "Epoch 36/100\n",
      "2793/2793 [==============================] - 0s 39us/step - loss: 1.8542 - acc: 0.2510 - val_loss: 1.8347 - val_acc: 0.2475\n",
      "Epoch 37/100\n",
      "2793/2793 [==============================] - 0s 42us/step - loss: 1.8536 - acc: 0.2499 - val_loss: 1.8327 - val_acc: 0.2561\n",
      "Epoch 38/100\n",
      "2793/2793 [==============================] - 0s 49us/step - loss: 1.8531 - acc: 0.2513 - val_loss: 1.8331 - val_acc: 0.2518\n",
      "Epoch 39/100\n",
      "2793/2793 [==============================] - 0s 41us/step - loss: 1.8523 - acc: 0.2513 - val_loss: 1.8325 - val_acc: 0.2475\n",
      "Epoch 40/100\n",
      "2793/2793 [==============================] - 0s 37us/step - loss: 1.8522 - acc: 0.2499 - val_loss: 1.8305 - val_acc: 0.2561\n",
      "Epoch 41/100\n",
      "2793/2793 [==============================] - 0s 39us/step - loss: 1.8518 - acc: 0.2503 - val_loss: 1.8309 - val_acc: 0.2561\n",
      "Epoch 42/100\n",
      "2793/2793 [==============================] - 0s 41us/step - loss: 1.8511 - acc: 0.2488 - val_loss: 1.8301 - val_acc: 0.2561\n",
      "Epoch 43/100\n",
      "2793/2793 [==============================] - 0s 47us/step - loss: 1.8506 - acc: 0.2513 - val_loss: 1.8298 - val_acc: 0.2575\n",
      "Epoch 44/100\n",
      "2793/2793 [==============================] - 0s 42us/step - loss: 1.8505 - acc: 0.2524 - val_loss: 1.8322 - val_acc: 0.2504\n",
      "Epoch 45/100\n",
      "2793/2793 [==============================] - 0s 38us/step - loss: 1.8502 - acc: 0.2517 - val_loss: 1.8297 - val_acc: 0.2561\n",
      "Epoch 46/100\n",
      "2793/2793 [==============================] - 0s 38us/step - loss: 1.8494 - acc: 0.2531 - val_loss: 1.8307 - val_acc: 0.2589\n",
      "Epoch 47/100\n",
      "2793/2793 [==============================] - 0s 40us/step - loss: 1.8491 - acc: 0.2524 - val_loss: 1.8290 - val_acc: 0.2504\n",
      "Epoch 48/100\n",
      "2793/2793 [==============================] - 0s 45us/step - loss: 1.8493 - acc: 0.2528 - val_loss: 1.8310 - val_acc: 0.2418\n",
      "Epoch 49/100\n",
      "2793/2793 [==============================] - 0s 43us/step - loss: 1.8486 - acc: 0.2517 - val_loss: 1.8276 - val_acc: 0.2575\n",
      "Epoch 50/100\n",
      "2793/2793 [==============================] - 0s 38us/step - loss: 1.8476 - acc: 0.2517 - val_loss: 1.8316 - val_acc: 0.2418\n",
      "Epoch 51/100\n",
      "2793/2793 [==============================] - 0s 37us/step - loss: 1.8479 - acc: 0.2503 - val_loss: 1.8283 - val_acc: 0.2446\n",
      "Epoch 52/100\n",
      "2793/2793 [==============================] - 0s 37us/step - loss: 1.8477 - acc: 0.2517 - val_loss: 1.8277 - val_acc: 0.2532\n",
      "Epoch 53/100\n",
      "2793/2793 [==============================] - 0s 40us/step - loss: 1.8469 - acc: 0.2528 - val_loss: 1.8318 - val_acc: 0.2418\n",
      "Epoch 54/100\n",
      "2793/2793 [==============================] - 0s 46us/step - loss: 1.8471 - acc: 0.2528 - val_loss: 1.8263 - val_acc: 0.2589\n",
      "Epoch 55/100\n",
      "2793/2793 [==============================] - 0s 44us/step - loss: 1.8467 - acc: 0.2510 - val_loss: 1.8298 - val_acc: 0.2546\n",
      "Epoch 56/100\n",
      "2793/2793 [==============================] - 0s 37us/step - loss: 1.8462 - acc: 0.2513 - val_loss: 1.8270 - val_acc: 0.2546\n",
      "Epoch 57/100\n",
      "2793/2793 [==============================] - 0s 39us/step - loss: 1.8459 - acc: 0.2517 - val_loss: 1.8282 - val_acc: 0.2518\n",
      "Epoch 58/100\n",
      "2793/2793 [==============================] - 0s 37us/step - loss: 1.8458 - acc: 0.2517 - val_loss: 1.8263 - val_acc: 0.2532\n",
      "Epoch 59/100\n",
      "2793/2793 [==============================] - 0s 39us/step - loss: 1.8453 - acc: 0.2513 - val_loss: 1.8242 - val_acc: 0.2546\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2793/2793 [==============================] - 0s 43us/step - loss: 1.8453 - acc: 0.2521 - val_loss: 1.8252 - val_acc: 0.2546\n",
      "Epoch 61/100\n",
      "2793/2793 [==============================] - 0s 45us/step - loss: 1.8447 - acc: 0.2510 - val_loss: 1.8251 - val_acc: 0.2546\n",
      "Epoch 62/100\n",
      "2793/2793 [==============================] - 0s 39us/step - loss: 1.8446 - acc: 0.2521 - val_loss: 1.8252 - val_acc: 0.2561\n",
      "Epoch 63/100\n",
      "2793/2793 [==============================] - 0s 39us/step - loss: 1.8444 - acc: 0.2513 - val_loss: 1.8253 - val_acc: 0.2546\n",
      "Epoch 64/100\n",
      "2793/2793 [==============================] - 0s 36us/step - loss: 1.8439 - acc: 0.2521 - val_loss: 1.8261 - val_acc: 0.2532\n",
      "Epoch 65/100\n",
      "2793/2793 [==============================] - 0s 38us/step - loss: 1.8438 - acc: 0.2528 - val_loss: 1.8243 - val_acc: 0.2575\n",
      "Epoch 66/100\n",
      "2793/2793 [==============================] - 0s 40us/step - loss: 1.8437 - acc: 0.2506 - val_loss: 1.8270 - val_acc: 0.2575\n",
      "Epoch 67/100\n",
      "2793/2793 [==============================] - 0s 44us/step - loss: 1.8434 - acc: 0.2524 - val_loss: 1.8272 - val_acc: 0.2589\n",
      "Epoch 68/100\n",
      "2793/2793 [==============================] - 0s 43us/step - loss: 1.8432 - acc: 0.2517 - val_loss: 1.8251 - val_acc: 0.2589\n",
      "Epoch 69/100\n",
      "2793/2793 [==============================] - 0s 39us/step - loss: 1.8428 - acc: 0.2517 - val_loss: 1.8235 - val_acc: 0.2604\n",
      "Epoch 70/100\n",
      "2793/2793 [==============================] - 0s 39us/step - loss: 1.8423 - acc: 0.2521 - val_loss: 1.8232 - val_acc: 0.2604\n",
      "Epoch 71/100\n",
      "2793/2793 [==============================] - 0s 39us/step - loss: 1.8424 - acc: 0.2524 - val_loss: 1.8233 - val_acc: 0.2604\n",
      "Epoch 72/100\n",
      "2793/2793 [==============================] - 0s 37us/step - loss: 1.8420 - acc: 0.2531 - val_loss: 1.8233 - val_acc: 0.2575\n",
      "Epoch 73/100\n",
      "2793/2793 [==============================] - 0s 40us/step - loss: 1.8419 - acc: 0.2506 - val_loss: 1.8242 - val_acc: 0.2546\n",
      "Epoch 74/100\n",
      "2793/2793 [==============================] - 0s 44us/step - loss: 1.8415 - acc: 0.2524 - val_loss: 1.8248 - val_acc: 0.2518\n",
      "Epoch 75/100\n",
      "2793/2793 [==============================] - 0s 44us/step - loss: 1.8414 - acc: 0.2517 - val_loss: 1.8241 - val_acc: 0.2532\n",
      "Epoch 76/100\n",
      "2793/2793 [==============================] - 0s 42us/step - loss: 1.8412 - acc: 0.2524 - val_loss: 1.8254 - val_acc: 0.2518\n",
      "Epoch 77/100\n",
      "2793/2793 [==============================] - 0s 37us/step - loss: 1.8410 - acc: 0.2528 - val_loss: 1.8240 - val_acc: 0.2504\n",
      "Epoch 78/100\n",
      "2793/2793 [==============================] - 0s 37us/step - loss: 1.8408 - acc: 0.2521 - val_loss: 1.8233 - val_acc: 0.2532\n",
      "Epoch 79/100\n",
      "2793/2793 [==============================] - 0s 40us/step - loss: 1.8413 - acc: 0.2524 - val_loss: 1.8237 - val_acc: 0.2589\n",
      "Epoch 80/100\n",
      "2793/2793 [==============================] - 0s 38us/step - loss: 1.8406 - acc: 0.2528 - val_loss: 1.8267 - val_acc: 0.2418\n",
      "Epoch 81/100\n",
      "2793/2793 [==============================] - 0s 40us/step - loss: 1.8401 - acc: 0.2524 - val_loss: 1.8247 - val_acc: 0.2532\n",
      "Epoch 82/100\n",
      "2793/2793 [==============================] - 0s 43us/step - loss: 1.8401 - acc: 0.2528 - val_loss: 1.8238 - val_acc: 0.2647\n",
      "Epoch 83/100\n",
      "2793/2793 [==============================] - 0s 46us/step - loss: 1.8398 - acc: 0.2535 - val_loss: 1.8226 - val_acc: 0.2604\n",
      "Epoch 84/100\n",
      "2793/2793 [==============================] - 0s 39us/step - loss: 1.8397 - acc: 0.2524 - val_loss: 1.8222 - val_acc: 0.2589\n",
      "Epoch 85/100\n",
      "2793/2793 [==============================] - 0s 43us/step - loss: 1.8394 - acc: 0.2513 - val_loss: 1.8238 - val_acc: 0.2532\n",
      "Epoch 86/100\n",
      "2793/2793 [==============================] - 0s 41us/step - loss: 1.8399 - acc: 0.2531 - val_loss: 1.8229 - val_acc: 0.2575\n",
      "Epoch 87/100\n",
      "2793/2793 [==============================] - 0s 37us/step - loss: 1.8389 - acc: 0.2521 - val_loss: 1.8232 - val_acc: 0.2632\n",
      "Epoch 88/100\n",
      "2793/2793 [==============================] - 0s 43us/step - loss: 1.8393 - acc: 0.2517 - val_loss: 1.8228 - val_acc: 0.2532\n",
      "Epoch 89/100\n",
      "2793/2793 [==============================] - 0s 44us/step - loss: 1.8388 - acc: 0.2528 - val_loss: 1.8214 - val_acc: 0.2604\n",
      "Epoch 90/100\n",
      "2793/2793 [==============================] - 0s 44us/step - loss: 1.8385 - acc: 0.2517 - val_loss: 1.8238 - val_acc: 0.2618\n",
      "Epoch 91/100\n",
      "2793/2793 [==============================] - 0s 44us/step - loss: 1.8383 - acc: 0.2517 - val_loss: 1.8222 - val_acc: 0.2604\n",
      "Epoch 92/100\n",
      "2793/2793 [==============================] - 0s 38us/step - loss: 1.8385 - acc: 0.2510 - val_loss: 1.8221 - val_acc: 0.2546\n",
      "Epoch 93/100\n",
      "2793/2793 [==============================] - 0s 44us/step - loss: 1.8378 - acc: 0.2542 - val_loss: 1.8214 - val_acc: 0.2604\n",
      "Epoch 94/100\n",
      "2793/2793 [==============================] - 0s 44us/step - loss: 1.8381 - acc: 0.2517 - val_loss: 1.8222 - val_acc: 0.2589\n",
      "Epoch 95/100\n",
      "2793/2793 [==============================] - 0s 42us/step - loss: 1.8378 - acc: 0.2524 - val_loss: 1.8211 - val_acc: 0.2575\n",
      "Epoch 96/100\n",
      "2793/2793 [==============================] - 0s 37us/step - loss: 1.8374 - acc: 0.2521 - val_loss: 1.8215 - val_acc: 0.2575\n",
      "Epoch 97/100\n",
      "2793/2793 [==============================] - 0s 39us/step - loss: 1.8375 - acc: 0.2531 - val_loss: 1.8215 - val_acc: 0.2589\n",
      "Epoch 98/100\n",
      "2793/2793 [==============================] - 0s 47us/step - loss: 1.8373 - acc: 0.2521 - val_loss: 1.8221 - val_acc: 0.2604\n",
      "Epoch 99/100\n",
      "2793/2793 [==============================] - 0s 43us/step - loss: 1.8369 - acc: 0.2524 - val_loss: 1.8210 - val_acc: 0.2618\n",
      "Epoch 100/100\n",
      "2793/2793 [==============================] - 0s 37us/step - loss: 1.8368 - acc: 0.2546 - val_loss: 1.8229 - val_acc: 0.2561\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e49668b630>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train model with training set, 100 epochs(100 times), verbose = 1 means we will see result after one epochs\n",
    "#validation by test_set\n",
    "model_valence.fit(X_training, Y_training_valence, epochs = 100, verbose = 1, validation_data=(X_test, Y_test_valence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2793 samples, validate on 699 samples\n",
      "Epoch 1/100\n",
      "2793/2793 [==============================] - 0s 116us/step - loss: 2.2862 - acc: 0.1697 - val_loss: 2.1282 - val_acc: 0.2561\n",
      "Epoch 2/100\n",
      "2793/2793 [==============================] - 0s 38us/step - loss: 2.0312 - acc: 0.2624 - val_loss: 1.9672 - val_acc: 0.2718\n",
      "Epoch 3/100\n",
      "2793/2793 [==============================] - 0s 52us/step - loss: 1.9275 - acc: 0.2610 - val_loss: 1.8995 - val_acc: 0.2647\n",
      "Epoch 4/100\n",
      "2793/2793 [==============================] - 0s 47us/step - loss: 1.8725 - acc: 0.2760 - val_loss: 1.8428 - val_acc: 0.2589\n",
      "Epoch 5/100\n",
      "2793/2793 [==============================] - 0s 50us/step - loss: 1.8403 - acc: 0.2760 - val_loss: 1.8149 - val_acc: 0.2718\n",
      "Epoch 6/100\n",
      "2793/2793 [==============================] - 0s 46us/step - loss: 1.8181 - acc: 0.2764 - val_loss: 1.7962 - val_acc: 0.2661\n",
      "Epoch 7/100\n",
      "2793/2793 [==============================] - 0s 40us/step - loss: 1.8016 - acc: 0.2725 - val_loss: 1.7772 - val_acc: 0.2518\n",
      "Epoch 8/100\n",
      "2793/2793 [==============================] - 0s 47us/step - loss: 1.7885 - acc: 0.2739 - val_loss: 1.7581 - val_acc: 0.2647\n",
      "Epoch 9/100\n",
      "2793/2793 [==============================] - 0s 42us/step - loss: 1.7798 - acc: 0.2775 - val_loss: 1.7510 - val_acc: 0.2632\n",
      "Epoch 10/100\n",
      "2793/2793 [==============================] - 0s 46us/step - loss: 1.7715 - acc: 0.2775 - val_loss: 1.7435 - val_acc: 0.2632\n",
      "Epoch 11/100\n",
      "2793/2793 [==============================] - 0s 47us/step - loss: 1.7658 - acc: 0.2764 - val_loss: 1.7374 - val_acc: 0.2604\n",
      "Epoch 12/100\n",
      "2793/2793 [==============================] - 0s 41us/step - loss: 1.7599 - acc: 0.2768 - val_loss: 1.7278 - val_acc: 0.2604\n",
      "Epoch 13/100\n",
      "2793/2793 [==============================] - 0s 44us/step - loss: 1.7559 - acc: 0.2760 - val_loss: 1.7269 - val_acc: 0.2618\n",
      "Epoch 14/100\n",
      "2793/2793 [==============================] - 0s 47us/step - loss: 1.7508 - acc: 0.2789 - val_loss: 1.7203 - val_acc: 0.2847\n",
      "Epoch 15/100\n",
      "2793/2793 [==============================] - 0s 39us/step - loss: 1.7480 - acc: 0.2789 - val_loss: 1.7177 - val_acc: 0.2847\n",
      "Epoch 16/100\n",
      "2793/2793 [==============================] - 0s 48us/step - loss: 1.7455 - acc: 0.2710 - val_loss: 1.7095 - val_acc: 0.2876\n",
      "Epoch 17/100\n",
      "2793/2793 [==============================] - 0s 47us/step - loss: 1.7420 - acc: 0.2750 - val_loss: 1.7110 - val_acc: 0.2704\n",
      "Epoch 18/100\n",
      "2793/2793 [==============================] - 0s 44us/step - loss: 1.7390 - acc: 0.2757 - val_loss: 1.7116 - val_acc: 0.2718\n",
      "Epoch 19/100\n",
      "2793/2793 [==============================] - 0s 39us/step - loss: 1.7367 - acc: 0.2728 - val_loss: 1.7019 - val_acc: 0.2833\n",
      "Epoch 20/100\n",
      "2793/2793 [==============================] - 0s 47us/step - loss: 1.7343 - acc: 0.2739 - val_loss: 1.7078 - val_acc: 0.2718\n",
      "Epoch 21/100\n",
      "2793/2793 [==============================] - 0s 45us/step - loss: 1.7332 - acc: 0.2775 - val_loss: 1.6981 - val_acc: 0.2804\n",
      "Epoch 22/100\n",
      "2793/2793 [==============================] - 0s 44us/step - loss: 1.7305 - acc: 0.2778 - val_loss: 1.6989 - val_acc: 0.2747\n",
      "Epoch 23/100\n",
      "2793/2793 [==============================] - 0s 47us/step - loss: 1.7289 - acc: 0.2775 - val_loss: 1.6952 - val_acc: 0.2775\n",
      "Epoch 24/100\n",
      "2793/2793 [==============================] - 0s 39us/step - loss: 1.7268 - acc: 0.2786 - val_loss: 1.6964 - val_acc: 0.2761\n",
      "Epoch 25/100\n",
      "2793/2793 [==============================] - 0s 48us/step - loss: 1.7256 - acc: 0.2743 - val_loss: 1.6927 - val_acc: 0.2918\n",
      "Epoch 26/100\n",
      "2793/2793 [==============================] - 0s 43us/step - loss: 1.7237 - acc: 0.2786 - val_loss: 1.6906 - val_acc: 0.2747\n",
      "Epoch 27/100\n",
      "2793/2793 [==============================] - 0s 37us/step - loss: 1.7230 - acc: 0.2789 - val_loss: 1.6865 - val_acc: 0.2804\n",
      "Epoch 28/100\n",
      "2793/2793 [==============================] - 0s 49us/step - loss: 1.7215 - acc: 0.2771 - val_loss: 1.6904 - val_acc: 0.2804\n",
      "Epoch 29/100\n",
      "2793/2793 [==============================] - 0s 42us/step - loss: 1.7209 - acc: 0.2782 - val_loss: 1.6948 - val_acc: 0.2747\n",
      "Epoch 30/100\n",
      "2793/2793 [==============================] - 0s 39us/step - loss: 1.7210 - acc: 0.2768 - val_loss: 1.6890 - val_acc: 0.2761\n",
      "Epoch 31/100\n",
      "2793/2793 [==============================] - 0s 45us/step - loss: 1.7188 - acc: 0.2728 - val_loss: 1.6917 - val_acc: 0.2790\n",
      "Epoch 32/100\n",
      "2793/2793 [==============================] - 0s 46us/step - loss: 1.7179 - acc: 0.2710 - val_loss: 1.6873 - val_acc: 0.2790\n",
      "Epoch 33/100\n",
      "2793/2793 [==============================] - 0s 38us/step - loss: 1.7165 - acc: 0.2757 - val_loss: 1.6860 - val_acc: 0.2904\n",
      "Epoch 34/100\n",
      "2793/2793 [==============================] - 0s 41us/step - loss: 1.7159 - acc: 0.2743 - val_loss: 1.6847 - val_acc: 0.2761\n",
      "Epoch 35/100\n",
      "2793/2793 [==============================] - 0s 49us/step - loss: 1.7140 - acc: 0.2750 - val_loss: 1.6846 - val_acc: 0.2775\n",
      "Epoch 36/100\n",
      "2793/2793 [==============================] - 0s 44us/step - loss: 1.7132 - acc: 0.2728 - val_loss: 1.6837 - val_acc: 0.2833\n",
      "Epoch 37/100\n",
      "2793/2793 [==============================] - 0s 47us/step - loss: 1.7128 - acc: 0.2739 - val_loss: 1.6790 - val_acc: 0.3004\n",
      "Epoch 38/100\n",
      "2793/2793 [==============================] - 0s 48us/step - loss: 1.7126 - acc: 0.2760 - val_loss: 1.6836 - val_acc: 0.2804\n",
      "Epoch 39/100\n",
      "2793/2793 [==============================] - 0s 56us/step - loss: 1.7111 - acc: 0.2750 - val_loss: 1.6811 - val_acc: 0.3004\n",
      "Epoch 40/100\n",
      "2793/2793 [==============================] - 0s 55us/step - loss: 1.7107 - acc: 0.2739 - val_loss: 1.6797 - val_acc: 0.2847\n",
      "Epoch 41/100\n",
      "2793/2793 [==============================] - 0s 46us/step - loss: 1.7089 - acc: 0.2768 - val_loss: 1.6814 - val_acc: 0.2804\n",
      "Epoch 42/100\n",
      "2793/2793 [==============================] - 0s 61us/step - loss: 1.7100 - acc: 0.2753 - val_loss: 1.6796 - val_acc: 0.2775\n",
      "Epoch 43/100\n",
      "2793/2793 [==============================] - 0s 57us/step - loss: 1.7088 - acc: 0.2739 - val_loss: 1.6786 - val_acc: 0.2933\n",
      "Epoch 44/100\n",
      "2793/2793 [==============================] - 0s 51us/step - loss: 1.7076 - acc: 0.2775 - val_loss: 1.6788 - val_acc: 0.2761\n",
      "Epoch 45/100\n",
      "2793/2793 [==============================] - 0s 50us/step - loss: 1.7070 - acc: 0.2746 - val_loss: 1.6769 - val_acc: 0.2933\n",
      "Epoch 46/100\n",
      "2793/2793 [==============================] - 0s 49us/step - loss: 1.7064 - acc: 0.2771 - val_loss: 1.6793 - val_acc: 0.2818\n",
      "Epoch 47/100\n",
      "2793/2793 [==============================] - 0s 58us/step - loss: 1.7069 - acc: 0.2768 - val_loss: 1.6753 - val_acc: 0.2961\n",
      "Epoch 48/100\n",
      "2793/2793 [==============================] - 0s 51us/step - loss: 1.7055 - acc: 0.2775 - val_loss: 1.6749 - val_acc: 0.2933\n",
      "Epoch 49/100\n",
      "2793/2793 [==============================] - 0s 54us/step - loss: 1.7048 - acc: 0.2768 - val_loss: 1.6772 - val_acc: 0.2833\n",
      "Epoch 50/100\n",
      "2793/2793 [==============================] - 0s 42us/step - loss: 1.7048 - acc: 0.2757 - val_loss: 1.6726 - val_acc: 0.2947\n",
      "Epoch 51/100\n",
      "2793/2793 [==============================] - 0s 46us/step - loss: 1.7034 - acc: 0.2782 - val_loss: 1.6757 - val_acc: 0.2775\n",
      "Epoch 52/100\n",
      "2793/2793 [==============================] - 0s 37us/step - loss: 1.7034 - acc: 0.2764 - val_loss: 1.6714 - val_acc: 0.2990\n",
      "Epoch 53/100\n",
      "2793/2793 [==============================] - 0s 45us/step - loss: 1.7033 - acc: 0.2818 - val_loss: 1.6753 - val_acc: 0.2790\n",
      "Epoch 54/100\n",
      "2793/2793 [==============================] - 0s 44us/step - loss: 1.7028 - acc: 0.2764 - val_loss: 1.6720 - val_acc: 0.2847\n",
      "Epoch 55/100\n",
      "2793/2793 [==============================] - 0s 37us/step - loss: 1.7022 - acc: 0.2760 - val_loss: 1.6722 - val_acc: 0.2990\n",
      "Epoch 56/100\n",
      "2793/2793 [==============================] - 0s 42us/step - loss: 1.7020 - acc: 0.2760 - val_loss: 1.6707 - val_acc: 0.2818\n",
      "Epoch 57/100\n",
      "2793/2793 [==============================] - 0s 44us/step - loss: 1.7015 - acc: 0.2739 - val_loss: 1.6766 - val_acc: 0.2790\n",
      "Epoch 58/100\n",
      "2793/2793 [==============================] - 0s 37us/step - loss: 1.7017 - acc: 0.2739 - val_loss: 1.6727 - val_acc: 0.2818\n",
      "Epoch 59/100\n",
      "2793/2793 [==============================] - 0s 42us/step - loss: 1.7010 - acc: 0.2764 - val_loss: 1.6738 - val_acc: 0.2790\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2793/2793 [==============================] - 0s 44us/step - loss: 1.7007 - acc: 0.2757 - val_loss: 1.6706 - val_acc: 0.2761\n",
      "Epoch 61/100\n",
      "2793/2793 [==============================] - 0s 37us/step - loss: 1.7012 - acc: 0.2764 - val_loss: 1.6697 - val_acc: 0.2976\n",
      "Epoch 62/100\n",
      "2793/2793 [==============================] - 0s 41us/step - loss: 1.7003 - acc: 0.2764 - val_loss: 1.6708 - val_acc: 0.2876\n",
      "Epoch 63/100\n",
      "2793/2793 [==============================] - 0s 44us/step - loss: 1.6997 - acc: 0.2739 - val_loss: 1.6711 - val_acc: 0.2833\n",
      "Epoch 64/100\n",
      "2793/2793 [==============================] - 0s 37us/step - loss: 1.6996 - acc: 0.2775 - val_loss: 1.6688 - val_acc: 0.3076\n",
      "Epoch 65/100\n",
      "2793/2793 [==============================] - 0s 39us/step - loss: 1.6985 - acc: 0.2768 - val_loss: 1.6726 - val_acc: 0.2861\n",
      "Epoch 66/100\n",
      "2793/2793 [==============================] - 0s 39us/step - loss: 1.6985 - acc: 0.2764 - val_loss: 1.6703 - val_acc: 0.2833\n",
      "Epoch 67/100\n",
      "2793/2793 [==============================] - 0s 44us/step - loss: 1.6978 - acc: 0.2778 - val_loss: 1.6674 - val_acc: 0.3004\n",
      "Epoch 68/100\n",
      "2793/2793 [==============================] - 0s 37us/step - loss: 1.6977 - acc: 0.2764 - val_loss: 1.6710 - val_acc: 0.2732\n",
      "Epoch 69/100\n",
      "2793/2793 [==============================] - 0s 38us/step - loss: 1.6975 - acc: 0.2811 - val_loss: 1.6699 - val_acc: 0.2876\n",
      "Epoch 70/100\n",
      "2793/2793 [==============================] - 0s 44us/step - loss: 1.6972 - acc: 0.2753 - val_loss: 1.6680 - val_acc: 0.2804\n",
      "Epoch 71/100\n",
      "2793/2793 [==============================] - 0s 40us/step - loss: 1.6965 - acc: 0.2771 - val_loss: 1.6681 - val_acc: 0.2990\n",
      "Epoch 72/100\n",
      "2793/2793 [==============================] - 0s 39us/step - loss: 1.6969 - acc: 0.2725 - val_loss: 1.6676 - val_acc: 0.2790\n",
      "Epoch 73/100\n",
      "2793/2793 [==============================] - 0s 38us/step - loss: 1.6959 - acc: 0.2764 - val_loss: 1.6690 - val_acc: 0.2861\n",
      "Epoch 74/100\n",
      "2793/2793 [==============================] - 0s 44us/step - loss: 1.6952 - acc: 0.2786 - val_loss: 1.6712 - val_acc: 0.2790\n",
      "Epoch 75/100\n",
      "2793/2793 [==============================] - 0s 39us/step - loss: 1.6957 - acc: 0.2771 - val_loss: 1.6698 - val_acc: 0.2775\n",
      "Epoch 76/100\n",
      "2793/2793 [==============================] - 0s 39us/step - loss: 1.6960 - acc: 0.2743 - val_loss: 1.6692 - val_acc: 0.2718\n",
      "Epoch 77/100\n",
      "2793/2793 [==============================] - 0s 44us/step - loss: 1.6972 - acc: 0.2739 - val_loss: 1.6661 - val_acc: 0.2761\n",
      "Epoch 78/100\n",
      "2793/2793 [==============================] - 0s 43us/step - loss: 1.6962 - acc: 0.2718 - val_loss: 1.6675 - val_acc: 0.3019\n",
      "Epoch 79/100\n",
      "2793/2793 [==============================] - 0s 37us/step - loss: 1.6959 - acc: 0.2786 - val_loss: 1.6675 - val_acc: 0.2961\n",
      "Epoch 80/100\n",
      "2793/2793 [==============================] - 0s 38us/step - loss: 1.6948 - acc: 0.2746 - val_loss: 1.6682 - val_acc: 0.2876\n",
      "Epoch 81/100\n",
      "2793/2793 [==============================] - 0s 38us/step - loss: 1.6943 - acc: 0.2789 - val_loss: 1.6647 - val_acc: 0.3019\n",
      "Epoch 82/100\n",
      "2793/2793 [==============================] - 0s 44us/step - loss: 1.6943 - acc: 0.2778 - val_loss: 1.6667 - val_acc: 0.2861\n",
      "Epoch 83/100\n",
      "2793/2793 [==============================] - 0s 41us/step - loss: 1.6935 - acc: 0.2775 - val_loss: 1.6644 - val_acc: 0.2947\n",
      "Epoch 84/100\n",
      "2793/2793 [==============================] - 0s 44us/step - loss: 1.6937 - acc: 0.2739 - val_loss: 1.6638 - val_acc: 0.3047\n",
      "Epoch 85/100\n",
      "2793/2793 [==============================] - 0s 48us/step - loss: 1.6936 - acc: 0.2778 - val_loss: 1.6620 - val_acc: 0.2876\n",
      "Epoch 86/100\n",
      "2793/2793 [==============================] - 0s 43us/step - loss: 1.6931 - acc: 0.2753 - val_loss: 1.6640 - val_acc: 0.2861\n",
      "Epoch 87/100\n",
      "2793/2793 [==============================] - 0s 53us/step - loss: 1.6923 - acc: 0.2710 - val_loss: 1.6647 - val_acc: 0.3004\n",
      "Epoch 88/100\n",
      "2793/2793 [==============================] - 0s 40us/step - loss: 1.6928 - acc: 0.2750 - val_loss: 1.6671 - val_acc: 0.2747\n",
      "Epoch 89/100\n",
      "2793/2793 [==============================] - 0s 44us/step - loss: 1.6925 - acc: 0.2778 - val_loss: 1.6649 - val_acc: 0.2876\n",
      "Epoch 90/100\n",
      "2793/2793 [==============================] - 0s 47us/step - loss: 1.6920 - acc: 0.2768 - val_loss: 1.6639 - val_acc: 0.2790\n",
      "Epoch 91/100\n",
      "2793/2793 [==============================] - 0s 38us/step - loss: 1.6916 - acc: 0.2768 - val_loss: 1.6619 - val_acc: 0.2976\n",
      "Epoch 92/100\n",
      "2793/2793 [==============================] - 0s 42us/step - loss: 1.6914 - acc: 0.2782 - val_loss: 1.6670 - val_acc: 0.2747\n",
      "Epoch 93/100\n",
      "2793/2793 [==============================] - 0s 50us/step - loss: 1.6926 - acc: 0.2753 - val_loss: 1.6680 - val_acc: 0.2790\n",
      "Epoch 94/100\n",
      "2793/2793 [==============================] - 0s 39us/step - loss: 1.6917 - acc: 0.2753 - val_loss: 1.6646 - val_acc: 0.2904\n",
      "Epoch 95/100\n",
      "2793/2793 [==============================] - 0s 41us/step - loss: 1.6910 - acc: 0.2775 - val_loss: 1.6681 - val_acc: 0.2775\n",
      "Epoch 96/100\n",
      "2793/2793 [==============================] - 0s 55us/step - loss: 1.6909 - acc: 0.2750 - val_loss: 1.6697 - val_acc: 0.2790\n",
      "Epoch 97/100\n",
      "2793/2793 [==============================] - 0s 39us/step - loss: 1.6913 - acc: 0.2775 - val_loss: 1.6629 - val_acc: 0.2918\n",
      "Epoch 98/100\n",
      "2793/2793 [==============================] - 0s 37us/step - loss: 1.6909 - acc: 0.2789 - val_loss: 1.6651 - val_acc: 0.2847\n",
      "Epoch 99/100\n",
      "2793/2793 [==============================] - 0s 45us/step - loss: 1.6906 - acc: 0.2793 - val_loss: 1.6617 - val_acc: 0.2976\n",
      "Epoch 100/100\n",
      "2793/2793 [==============================] - 0s 47us/step - loss: 1.6901 - acc: 0.2757 - val_loss: 1.6640 - val_acc: 0.2775\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e49668b5f8>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_arousal.fit(X_training, Y_training_arousal, epochs = 100, verbose = 1, validation_data=(X_test, Y_test_arousal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Design neutral network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We add more layer into network and using activation tanh\n",
    "#layer dense and BatchNormalization\n",
    "model_valence = Sequential()\n",
    "model_valence.add(Dense(64, activation = 'tanh', input_shape = (4,)))\n",
    "model_valence.add(BatchNormalization())\n",
    "model_valence.add(Dense(64, activation = 'tanh'))\n",
    "model_valence.add(BatchNormalization())\n",
    "model_valence.add(Dense(64, activation = 'tanh'))\n",
    "model_valence.add(BatchNormalization())\n",
    "model_valence.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model_arousal = Sequential()\n",
    "model_arousal.add(Dense(64, activation = 'tanh', input_shape = (4,)))\n",
    "model_arousal.add(BatchNormalization())\n",
    "model_arousal.add(Dense(64, activation = 'tanh'))\n",
    "model_arousal.add(BatchNormalization())\n",
    "model_arousal.add(Dense(64, activation = 'tanh'))\n",
    "model_arousal.add(BatchNormalization())\n",
    "model_arousal.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuration model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we use loss = mean squared error\n",
    "#optimizer by adam which allows us to decrease learning rate overtime\n",
    "#optimize with accuracy\n",
    "model_valence.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "model_arousal.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2793 samples, validate on 699 samples\n",
      "Epoch 1/100\n",
      "2793/2793 [==============================] - 1s 497us/step - loss: 0.0906 - acc: 0.1880 - val_loss: 0.0879 - val_acc: 0.2318\n",
      "Epoch 2/100\n",
      "2793/2793 [==============================] - 0s 72us/step - loss: 0.0870 - acc: 0.2055 - val_loss: 0.0847 - val_acc: 0.2418\n",
      "Epoch 3/100\n",
      "2793/2793 [==============================] - 0s 77us/step - loss: 0.0853 - acc: 0.2245 - val_loss: 0.0858 - val_acc: 0.2160\n",
      "Epoch 4/100\n",
      "2793/2793 [==============================] - 0s 73us/step - loss: 0.0845 - acc: 0.2331 - val_loss: 0.0841 - val_acc: 0.2246\n",
      "Epoch 5/100\n",
      "2793/2793 [==============================] - 0s 67us/step - loss: 0.0842 - acc: 0.2159 - val_loss: 0.0841 - val_acc: 0.2189\n",
      "Epoch 6/100\n",
      "2793/2793 [==============================] - 0s 67us/step - loss: 0.0833 - acc: 0.2460 - val_loss: 0.0837 - val_acc: 0.2175\n",
      "Epoch 7/100\n",
      "2793/2793 [==============================] - 0s 71us/step - loss: 0.0831 - acc: 0.2345 - val_loss: 0.0832 - val_acc: 0.2375\n",
      "Epoch 8/100\n",
      "2793/2793 [==============================] - 0s 69us/step - loss: 0.0829 - acc: 0.2381 - val_loss: 0.0829 - val_acc: 0.2260\n",
      "Epoch 9/100\n",
      "2793/2793 [==============================] - 0s 73us/step - loss: 0.0829 - acc: 0.2342 - val_loss: 0.0828 - val_acc: 0.2461\n",
      "Epoch 10/100\n",
      "2793/2793 [==============================] - 0s 67us/step - loss: 0.0829 - acc: 0.2277 - val_loss: 0.0834 - val_acc: 0.2332\n",
      "Epoch 11/100\n",
      "2793/2793 [==============================] - 0s 68us/step - loss: 0.0829 - acc: 0.2388 - val_loss: 0.0827 - val_acc: 0.2275\n",
      "Epoch 12/100\n",
      "2793/2793 [==============================] - 0s 71us/step - loss: 0.0827 - acc: 0.2334 - val_loss: 0.0825 - val_acc: 0.2346\n",
      "Epoch 13/100\n",
      "2793/2793 [==============================] - 0s 70us/step - loss: 0.0826 - acc: 0.2402 - val_loss: 0.0831 - val_acc: 0.2303\n",
      "Epoch 14/100\n",
      "2793/2793 [==============================] - 0s 74us/step - loss: 0.0826 - acc: 0.2406 - val_loss: 0.0828 - val_acc: 0.2446\n",
      "Epoch 15/100\n",
      "2793/2793 [==============================] - 0s 75us/step - loss: 0.0827 - acc: 0.2420 - val_loss: 0.0829 - val_acc: 0.2418\n",
      "Epoch 16/100\n",
      "2793/2793 [==============================] - 0s 69us/step - loss: 0.0827 - acc: 0.2410 - val_loss: 0.0827 - val_acc: 0.2446\n",
      "Epoch 17/100\n",
      "2793/2793 [==============================] - 0s 72us/step - loss: 0.0827 - acc: 0.2460 - val_loss: 0.0828 - val_acc: 0.2446\n",
      "Epoch 18/100\n",
      "2793/2793 [==============================] - 0s 66us/step - loss: 0.0825 - acc: 0.2374 - val_loss: 0.0828 - val_acc: 0.2461\n",
      "Epoch 19/100\n",
      "2793/2793 [==============================] - 0s 74us/step - loss: 0.0826 - acc: 0.2420 - val_loss: 0.0827 - val_acc: 0.2446\n",
      "Epoch 20/100\n",
      "2793/2793 [==============================] - 0s 72us/step - loss: 0.0827 - acc: 0.2410 - val_loss: 0.0827 - val_acc: 0.2418\n",
      "Epoch 21/100\n",
      "2793/2793 [==============================] - 0s 72us/step - loss: 0.0826 - acc: 0.2406 - val_loss: 0.0830 - val_acc: 0.1931\n",
      "Epoch 22/100\n",
      "2793/2793 [==============================] - 0s 67us/step - loss: 0.0825 - acc: 0.2453 - val_loss: 0.0831 - val_acc: 0.2432\n",
      "Epoch 23/100\n",
      "2793/2793 [==============================] - 0s 71us/step - loss: 0.0827 - acc: 0.2420 - val_loss: 0.0833 - val_acc: 0.2418\n",
      "Epoch 24/100\n",
      "2793/2793 [==============================] - 0s 70us/step - loss: 0.0825 - acc: 0.2420 - val_loss: 0.0829 - val_acc: 0.2332\n",
      "Epoch 25/100\n",
      "2793/2793 [==============================] - 0s 83us/step - loss: 0.0825 - acc: 0.2306 - val_loss: 0.0830 - val_acc: 0.2361\n",
      "Epoch 26/100\n",
      "2793/2793 [==============================] - 0s 79us/step - loss: 0.0826 - acc: 0.2410 - val_loss: 0.0829 - val_acc: 0.2403\n",
      "Epoch 27/100\n",
      "2793/2793 [==============================] - 0s 83us/step - loss: 0.0826 - acc: 0.2470 - val_loss: 0.0828 - val_acc: 0.2031\n",
      "Epoch 28/100\n",
      "2793/2793 [==============================] - 0s 80us/step - loss: 0.0824 - acc: 0.2427 - val_loss: 0.0829 - val_acc: 0.2446\n",
      "Epoch 29/100\n",
      "2793/2793 [==============================] - 0s 77us/step - loss: 0.0824 - acc: 0.2481 - val_loss: 0.0830 - val_acc: 0.2375\n",
      "Epoch 30/100\n",
      "2793/2793 [==============================] - 0s 70us/step - loss: 0.0825 - acc: 0.2381 - val_loss: 0.0827 - val_acc: 0.2446\n",
      "Epoch 31/100\n",
      "2793/2793 [==============================] - 0s 77us/step - loss: 0.0826 - acc: 0.2427 - val_loss: 0.0829 - val_acc: 0.2346\n",
      "Epoch 32/100\n",
      "2793/2793 [==============================] - 0s 69us/step - loss: 0.0825 - acc: 0.2453 - val_loss: 0.0827 - val_acc: 0.2446\n",
      "Epoch 33/100\n",
      "2793/2793 [==============================] - 0s 84us/step - loss: 0.0826 - acc: 0.2427 - val_loss: 0.0829 - val_acc: 0.2375\n",
      "Epoch 34/100\n",
      "2793/2793 [==============================] - 0s 75us/step - loss: 0.0824 - acc: 0.2492 - val_loss: 0.0826 - val_acc: 0.2446\n",
      "Epoch 35/100\n",
      "2793/2793 [==============================] - 0s 83us/step - loss: 0.0826 - acc: 0.2363 - val_loss: 0.0825 - val_acc: 0.2446\n",
      "Epoch 36/100\n",
      "2793/2793 [==============================] - 0s 77us/step - loss: 0.0826 - acc: 0.2445 - val_loss: 0.0828 - val_acc: 0.2432\n",
      "Epoch 37/100\n",
      "2793/2793 [==============================] - 0s 90us/step - loss: 0.0824 - acc: 0.2453 - val_loss: 0.0830 - val_acc: 0.2504\n",
      "Epoch 38/100\n",
      "2793/2793 [==============================] - 0s 81us/step - loss: 0.0825 - acc: 0.2399 - val_loss: 0.0828 - val_acc: 0.2446\n",
      "Epoch 39/100\n",
      "2793/2793 [==============================] - 0s 73us/step - loss: 0.0825 - acc: 0.2406 - val_loss: 0.0828 - val_acc: 0.2446\n",
      "Epoch 40/100\n",
      "2793/2793 [==============================] - 0s 72us/step - loss: 0.0825 - acc: 0.2449 - val_loss: 0.0830 - val_acc: 0.2074\n",
      "Epoch 41/100\n",
      "2793/2793 [==============================] - 0s 67us/step - loss: 0.0825 - acc: 0.2453 - val_loss: 0.0828 - val_acc: 0.2461\n",
      "Epoch 42/100\n",
      "2793/2793 [==============================] - 0s 80us/step - loss: 0.0824 - acc: 0.2431 - val_loss: 0.0832 - val_acc: 0.2246\n",
      "Epoch 43/100\n",
      "2793/2793 [==============================] - 0s 69us/step - loss: 0.0825 - acc: 0.2474 - val_loss: 0.0830 - val_acc: 0.2446\n",
      "Epoch 44/100\n",
      "2793/2793 [==============================] - 0s 73us/step - loss: 0.0824 - acc: 0.2442 - val_loss: 0.0830 - val_acc: 0.2446\n",
      "Epoch 45/100\n",
      "2793/2793 [==============================] - 0s 75us/step - loss: 0.0823 - acc: 0.2506 - val_loss: 0.0833 - val_acc: 0.2046\n",
      "Epoch 46/100\n",
      "2793/2793 [==============================] - 0s 72us/step - loss: 0.0824 - acc: 0.2445 - val_loss: 0.0831 - val_acc: 0.2289\n",
      "Epoch 47/100\n",
      "2793/2793 [==============================] - 0s 68us/step - loss: 0.0824 - acc: 0.2456 - val_loss: 0.0829 - val_acc: 0.2361\n",
      "Epoch 48/100\n",
      "2793/2793 [==============================] - 0s 76us/step - loss: 0.0826 - acc: 0.2413 - val_loss: 0.0828 - val_acc: 0.2418\n",
      "Epoch 49/100\n",
      "2793/2793 [==============================] - 0s 72us/step - loss: 0.0824 - acc: 0.2442 - val_loss: 0.0831 - val_acc: 0.2346\n",
      "Epoch 50/100\n",
      "2793/2793 [==============================] - 0s 67us/step - loss: 0.0826 - acc: 0.2392 - val_loss: 0.0826 - val_acc: 0.2589\n",
      "Epoch 51/100\n",
      "2793/2793 [==============================] - 0s 74us/step - loss: 0.0825 - acc: 0.2359 - val_loss: 0.0827 - val_acc: 0.2446\n",
      "Epoch 52/100\n",
      "2793/2793 [==============================] - 0s 73us/step - loss: 0.0825 - acc: 0.2438 - val_loss: 0.0827 - val_acc: 0.2446\n",
      "Epoch 53/100\n",
      "2793/2793 [==============================] - 0s 69us/step - loss: 0.0824 - acc: 0.2427 - val_loss: 0.0828 - val_acc: 0.2418\n",
      "Epoch 54/100\n",
      "2793/2793 [==============================] - 0s 69us/step - loss: 0.0825 - acc: 0.2445 - val_loss: 0.0829 - val_acc: 0.2446\n",
      "Epoch 55/100\n",
      "2793/2793 [==============================] - 0s 76us/step - loss: 0.0824 - acc: 0.2420 - val_loss: 0.0828 - val_acc: 0.2432\n",
      "Epoch 56/100\n",
      "2793/2793 [==============================] - 0s 67us/step - loss: 0.0826 - acc: 0.2470 - val_loss: 0.0828 - val_acc: 0.2432\n",
      "Epoch 57/100\n",
      "2793/2793 [==============================] - 0s 67us/step - loss: 0.0824 - acc: 0.2478 - val_loss: 0.0831 - val_acc: 0.2403\n",
      "Epoch 58/100\n",
      "2793/2793 [==============================] - 0s 75us/step - loss: 0.0824 - acc: 0.2488 - val_loss: 0.0831 - val_acc: 0.2446\n",
      "Epoch 59/100\n",
      "2793/2793 [==============================] - 0s 78us/step - loss: 0.0824 - acc: 0.2392 - val_loss: 0.0829 - val_acc: 0.2446\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2793/2793 [==============================] - 0s 72us/step - loss: 0.0824 - acc: 0.2427 - val_loss: 0.0829 - val_acc: 0.2461\n",
      "Epoch 61/100\n",
      "2793/2793 [==============================] - 0s 67us/step - loss: 0.0824 - acc: 0.2506 - val_loss: 0.0830 - val_acc: 0.2418\n",
      "Epoch 62/100\n",
      "2793/2793 [==============================] - 0s 75us/step - loss: 0.0824 - acc: 0.2420 - val_loss: 0.0830 - val_acc: 0.2346\n",
      "Epoch 63/100\n",
      "2793/2793 [==============================] - 0s 78us/step - loss: 0.0824 - acc: 0.2456 - val_loss: 0.0832 - val_acc: 0.2289\n",
      "Epoch 64/100\n",
      "2793/2793 [==============================] - 0s 75us/step - loss: 0.0825 - acc: 0.2410 - val_loss: 0.0829 - val_acc: 0.2418\n",
      "Epoch 65/100\n",
      "2793/2793 [==============================] - 0s 74us/step - loss: 0.0823 - acc: 0.2449 - val_loss: 0.0830 - val_acc: 0.2375\n",
      "Epoch 66/100\n",
      "2793/2793 [==============================] - 0s 69us/step - loss: 0.0824 - acc: 0.2481 - val_loss: 0.0831 - val_acc: 0.2446\n",
      "Epoch 67/100\n",
      "2793/2793 [==============================] - 0s 78us/step - loss: 0.0825 - acc: 0.2431 - val_loss: 0.0830 - val_acc: 0.2461\n",
      "Epoch 68/100\n",
      "2793/2793 [==============================] - 0s 69us/step - loss: 0.0824 - acc: 0.2499 - val_loss: 0.0831 - val_acc: 0.2446\n",
      "Epoch 69/100\n",
      "2793/2793 [==============================] - 0s 74us/step - loss: 0.0825 - acc: 0.2460 - val_loss: 0.0829 - val_acc: 0.2403\n",
      "Epoch 70/100\n",
      "2793/2793 [==============================] - 0s 74us/step - loss: 0.0824 - acc: 0.2453 - val_loss: 0.0829 - val_acc: 0.2446\n",
      "Epoch 71/100\n",
      "2793/2793 [==============================] - 0s 73us/step - loss: 0.0824 - acc: 0.2528 - val_loss: 0.0830 - val_acc: 0.2060\n",
      "Epoch 72/100\n",
      "2793/2793 [==============================] - 0s 73us/step - loss: 0.0824 - acc: 0.2406 - val_loss: 0.0829 - val_acc: 0.2403\n",
      "Epoch 73/100\n",
      "2793/2793 [==============================] - 0s 77us/step - loss: 0.0824 - acc: 0.2431 - val_loss: 0.0829 - val_acc: 0.2332\n",
      "Epoch 74/100\n",
      "2793/2793 [==============================] - 0s 78us/step - loss: 0.0825 - acc: 0.2438 - val_loss: 0.0829 - val_acc: 0.2361\n",
      "Epoch 75/100\n",
      "2793/2793 [==============================] - 0s 79us/step - loss: 0.0824 - acc: 0.2424 - val_loss: 0.0830 - val_acc: 0.2089\n",
      "Epoch 76/100\n",
      "2793/2793 [==============================] - 0s 80us/step - loss: 0.0824 - acc: 0.2427 - val_loss: 0.0828 - val_acc: 0.2403\n",
      "Epoch 77/100\n",
      "2793/2793 [==============================] - 0s 81us/step - loss: 0.0824 - acc: 0.2463 - val_loss: 0.0831 - val_acc: 0.2418\n",
      "Epoch 78/100\n",
      "2793/2793 [==============================] - 0s 81us/step - loss: 0.0824 - acc: 0.2524 - val_loss: 0.0828 - val_acc: 0.2418\n",
      "Epoch 79/100\n",
      "2793/2793 [==============================] - 0s 74us/step - loss: 0.0823 - acc: 0.2517 - val_loss: 0.0832 - val_acc: 0.2361\n",
      "Epoch 80/100\n",
      "2793/2793 [==============================] - 0s 75us/step - loss: 0.0825 - acc: 0.2413 - val_loss: 0.0831 - val_acc: 0.2361\n",
      "Epoch 81/100\n",
      "2793/2793 [==============================] - 0s 78us/step - loss: 0.0824 - acc: 0.2431 - val_loss: 0.0833 - val_acc: 0.2432\n",
      "Epoch 82/100\n",
      "2793/2793 [==============================] - 0s 76us/step - loss: 0.0823 - acc: 0.2546 - val_loss: 0.0838 - val_acc: 0.2389\n",
      "Epoch 83/100\n",
      "2793/2793 [==============================] - 0s 73us/step - loss: 0.0826 - acc: 0.2363 - val_loss: 0.0833 - val_acc: 0.2303\n",
      "Epoch 84/100\n",
      "2793/2793 [==============================] - 0s 73us/step - loss: 0.0825 - acc: 0.2463 - val_loss: 0.0833 - val_acc: 0.2289\n",
      "Epoch 85/100\n",
      "2793/2793 [==============================] - 0s 76us/step - loss: 0.0825 - acc: 0.2413 - val_loss: 0.0835 - val_acc: 0.2361\n",
      "Epoch 86/100\n",
      "2793/2793 [==============================] - 0s 70us/step - loss: 0.0825 - acc: 0.2431 - val_loss: 0.0831 - val_acc: 0.2332\n",
      "Epoch 87/100\n",
      "2793/2793 [==============================] - 0s 78us/step - loss: 0.0825 - acc: 0.2284 - val_loss: 0.0831 - val_acc: 0.2175\n",
      "Epoch 88/100\n",
      "2793/2793 [==============================] - 0s 70us/step - loss: 0.0824 - acc: 0.2478 - val_loss: 0.0831 - val_acc: 0.2375\n",
      "Epoch 89/100\n",
      "2793/2793 [==============================] - 0s 75us/step - loss: 0.0824 - acc: 0.2410 - val_loss: 0.0834 - val_acc: 0.2146\n",
      "Epoch 90/100\n",
      "2793/2793 [==============================] - 0s 78us/step - loss: 0.0824 - acc: 0.2531 - val_loss: 0.0830 - val_acc: 0.2418\n",
      "Epoch 91/100\n",
      "2793/2793 [==============================] - 0s 77us/step - loss: 0.0824 - acc: 0.2427 - val_loss: 0.0830 - val_acc: 0.2403\n",
      "Epoch 92/100\n",
      "2793/2793 [==============================] - 0s 67us/step - loss: 0.0824 - acc: 0.2485 - val_loss: 0.0830 - val_acc: 0.2418\n",
      "Epoch 93/100\n",
      "2793/2793 [==============================] - 0s 79us/step - loss: 0.0825 - acc: 0.2470 - val_loss: 0.0830 - val_acc: 0.2418\n",
      "Epoch 94/100\n",
      "2793/2793 [==============================] - 0s 68us/step - loss: 0.0823 - acc: 0.2460 - val_loss: 0.0834 - val_acc: 0.2361\n",
      "Epoch 95/100\n",
      "2793/2793 [==============================] - 0s 68us/step - loss: 0.0824 - acc: 0.2395 - val_loss: 0.0831 - val_acc: 0.2418\n",
      "Epoch 96/100\n",
      "2793/2793 [==============================] - 0s 80us/step - loss: 0.0823 - acc: 0.2492 - val_loss: 0.0833 - val_acc: 0.2132\n",
      "Epoch 97/100\n",
      "2793/2793 [==============================] - 0s 67us/step - loss: 0.0822 - acc: 0.2549 - val_loss: 0.0831 - val_acc: 0.2418\n",
      "Epoch 98/100\n",
      "2793/2793 [==============================] - 0s 68us/step - loss: 0.0822 - acc: 0.2517 - val_loss: 0.0833 - val_acc: 0.2432\n",
      "Epoch 99/100\n",
      "2793/2793 [==============================] - 0s 77us/step - loss: 0.0824 - acc: 0.2485 - val_loss: 0.0835 - val_acc: 0.2361\n",
      "Epoch 100/100\n",
      "2793/2793 [==============================] - 0s 71us/step - loss: 0.0824 - acc: 0.2410 - val_loss: 0.0829 - val_acc: 0.2418\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e49181fba8>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train model with training set, 100 epochs(100 times), verbose = 1 means we will see result after one epochs\n",
    "#validation by test_set\n",
    "model_valence.fit(X_training, Y_training_valence, epochs = 100, verbose = 1, validation_data=(X_test, Y_test_valence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2793 samples, validate on 699 samples\n",
      "Epoch 1/100\n",
      "2793/2793 [==============================] - 1s 466us/step - loss: 0.0908 - acc: 0.2213 - val_loss: 0.0880 - val_acc: 0.2017\n",
      "Epoch 2/100\n",
      "2793/2793 [==============================] - 0s 77us/step - loss: 0.0866 - acc: 0.2220 - val_loss: 0.0835 - val_acc: 0.2618\n",
      "Epoch 3/100\n",
      "2793/2793 [==============================] - 0s 76us/step - loss: 0.0839 - acc: 0.2496 - val_loss: 0.0829 - val_acc: 0.2504\n",
      "Epoch 4/100\n",
      "2793/2793 [==============================] - 0s 71us/step - loss: 0.0827 - acc: 0.2517 - val_loss: 0.0814 - val_acc: 0.2661\n",
      "Epoch 5/100\n",
      "2793/2793 [==============================] - 0s 80us/step - loss: 0.0822 - acc: 0.2549 - val_loss: 0.0810 - val_acc: 0.2933\n",
      "Epoch 6/100\n",
      "2793/2793 [==============================] - 0s 69us/step - loss: 0.0814 - acc: 0.2542 - val_loss: 0.0802 - val_acc: 0.2961\n",
      "Epoch 7/100\n",
      "2793/2793 [==============================] - 0s 78us/step - loss: 0.0806 - acc: 0.2621 - val_loss: 0.0803 - val_acc: 0.2747\n",
      "Epoch 8/100\n",
      "2793/2793 [==============================] - 0s 71us/step - loss: 0.0802 - acc: 0.2574 - val_loss: 0.0800 - val_acc: 0.2690\n",
      "Epoch 9/100\n",
      "2793/2793 [==============================] - 0s 76us/step - loss: 0.0801 - acc: 0.2732 - val_loss: 0.0799 - val_acc: 0.2990\n",
      "Epoch 10/100\n",
      "2793/2793 [==============================] - 0s 79us/step - loss: 0.0799 - acc: 0.2732 - val_loss: 0.0801 - val_acc: 0.2532\n",
      "Epoch 11/100\n",
      "2793/2793 [==============================] - 0s 79us/step - loss: 0.0799 - acc: 0.2653 - val_loss: 0.0801 - val_acc: 0.2790\n",
      "Epoch 12/100\n",
      "2793/2793 [==============================] - 0s 75us/step - loss: 0.0799 - acc: 0.2789 - val_loss: 0.0800 - val_acc: 0.2775\n",
      "Epoch 13/100\n",
      "2793/2793 [==============================] - 0s 77us/step - loss: 0.0798 - acc: 0.2707 - val_loss: 0.0798 - val_acc: 0.2618\n",
      "Epoch 14/100\n",
      "2793/2793 [==============================] - 0s 82us/step - loss: 0.0800 - acc: 0.2621 - val_loss: 0.0800 - val_acc: 0.2718\n",
      "Epoch 15/100\n",
      "2793/2793 [==============================] - 0s 81us/step - loss: 0.0798 - acc: 0.2710 - val_loss: 0.0804 - val_acc: 0.2661\n",
      "Epoch 16/100\n",
      "2793/2793 [==============================] - 0s 75us/step - loss: 0.0798 - acc: 0.2768 - val_loss: 0.0802 - val_acc: 0.2675\n",
      "Epoch 17/100\n",
      "2793/2793 [==============================] - 0s 75us/step - loss: 0.0797 - acc: 0.2696 - val_loss: 0.0803 - val_acc: 0.2732\n",
      "Epoch 18/100\n",
      "2793/2793 [==============================] - 0s 74us/step - loss: 0.0798 - acc: 0.2692 - val_loss: 0.0802 - val_acc: 0.2732\n",
      "Epoch 19/100\n",
      "2793/2793 [==============================] - 0s 83us/step - loss: 0.0799 - acc: 0.2678 - val_loss: 0.0808 - val_acc: 0.2504\n",
      "Epoch 20/100\n",
      "2793/2793 [==============================] - 0s 79us/step - loss: 0.0798 - acc: 0.2682 - val_loss: 0.0802 - val_acc: 0.2761\n",
      "Epoch 21/100\n",
      "2793/2793 [==============================] - 0s 80us/step - loss: 0.0798 - acc: 0.2664 - val_loss: 0.0807 - val_acc: 0.2375\n",
      "Epoch 22/100\n",
      "2793/2793 [==============================] - 0s 74us/step - loss: 0.0798 - acc: 0.2671 - val_loss: 0.0804 - val_acc: 0.2475\n",
      "Epoch 23/100\n",
      "2793/2793 [==============================] - 0s 75us/step - loss: 0.0798 - acc: 0.2675 - val_loss: 0.0803 - val_acc: 0.2690\n",
      "Epoch 24/100\n",
      "2793/2793 [==============================] - 0s 80us/step - loss: 0.0796 - acc: 0.2714 - val_loss: 0.0800 - val_acc: 0.2647\n",
      "Epoch 25/100\n",
      "2793/2793 [==============================] - 0s 71us/step - loss: 0.0798 - acc: 0.2689 - val_loss: 0.0801 - val_acc: 0.2732\n",
      "Epoch 26/100\n",
      "2793/2793 [==============================] - 0s 85us/step - loss: 0.0796 - acc: 0.2807 - val_loss: 0.0803 - val_acc: 0.2790\n",
      "Epoch 27/100\n",
      "2793/2793 [==============================] - 0s 70us/step - loss: 0.0798 - acc: 0.2750 - val_loss: 0.0802 - val_acc: 0.2690\n",
      "Epoch 28/100\n",
      "2793/2793 [==============================] - 0s 80us/step - loss: 0.0796 - acc: 0.2700 - val_loss: 0.0805 - val_acc: 0.2718\n",
      "Epoch 29/100\n",
      "2793/2793 [==============================] - 0s 67us/step - loss: 0.0797 - acc: 0.2728 - val_loss: 0.0807 - val_acc: 0.2461\n",
      "Epoch 30/100\n",
      "2793/2793 [==============================] - 0s 78us/step - loss: 0.0795 - acc: 0.2750 - val_loss: 0.0805 - val_acc: 0.2675\n",
      "Epoch 31/100\n",
      "2793/2793 [==============================] - 0s 71us/step - loss: 0.0798 - acc: 0.2818 - val_loss: 0.0802 - val_acc: 0.2661\n",
      "Epoch 32/100\n",
      "2793/2793 [==============================] - 0s 75us/step - loss: 0.0795 - acc: 0.2757 - val_loss: 0.0803 - val_acc: 0.2675\n",
      "Epoch 33/100\n",
      "2793/2793 [==============================] - 0s 77us/step - loss: 0.0797 - acc: 0.2714 - val_loss: 0.0802 - val_acc: 0.2661\n",
      "Epoch 34/100\n",
      "2793/2793 [==============================] - 0s 74us/step - loss: 0.0796 - acc: 0.2757 - val_loss: 0.0802 - val_acc: 0.2704\n",
      "Epoch 35/100\n",
      "2793/2793 [==============================] - 0s 74us/step - loss: 0.0797 - acc: 0.2685 - val_loss: 0.0800 - val_acc: 0.2690\n",
      "Epoch 36/100\n",
      "2793/2793 [==============================] - 0s 72us/step - loss: 0.0795 - acc: 0.2746 - val_loss: 0.0803 - val_acc: 0.2647\n",
      "Epoch 37/100\n",
      "2793/2793 [==============================] - 0s 76us/step - loss: 0.0796 - acc: 0.2814 - val_loss: 0.0800 - val_acc: 0.2675\n",
      "Epoch 38/100\n",
      "2793/2793 [==============================] - 0s 75us/step - loss: 0.0795 - acc: 0.2718 - val_loss: 0.0806 - val_acc: 0.2675\n",
      "Epoch 39/100\n",
      "2793/2793 [==============================] - 0s 73us/step - loss: 0.0796 - acc: 0.2739 - val_loss: 0.0803 - val_acc: 0.2704\n",
      "Epoch 40/100\n",
      "2793/2793 [==============================] - 0s 76us/step - loss: 0.0798 - acc: 0.2632 - val_loss: 0.0805 - val_acc: 0.2647\n",
      "Epoch 41/100\n",
      "2793/2793 [==============================] - 0s 72us/step - loss: 0.0798 - acc: 0.2646 - val_loss: 0.0804 - val_acc: 0.2546\n",
      "Epoch 42/100\n",
      "2793/2793 [==============================] - 0s 74us/step - loss: 0.0796 - acc: 0.2728 - val_loss: 0.0803 - val_acc: 0.2747\n",
      "Epoch 43/100\n",
      "2793/2793 [==============================] - 0s 76us/step - loss: 0.0796 - acc: 0.2743 - val_loss: 0.0804 - val_acc: 0.2647\n",
      "Epoch 44/100\n",
      "2793/2793 [==============================] - 0s 70us/step - loss: 0.0796 - acc: 0.2775 - val_loss: 0.0805 - val_acc: 0.2704\n",
      "Epoch 45/100\n",
      "2793/2793 [==============================] - 0s 76us/step - loss: 0.0796 - acc: 0.2689 - val_loss: 0.0805 - val_acc: 0.2718\n",
      "Epoch 46/100\n",
      "2793/2793 [==============================] - 0s 75us/step - loss: 0.0796 - acc: 0.2671 - val_loss: 0.0800 - val_acc: 0.2761\n",
      "Epoch 47/100\n",
      "2793/2793 [==============================] - 0s 73us/step - loss: 0.0796 - acc: 0.2675 - val_loss: 0.0802 - val_acc: 0.2675\n",
      "Epoch 48/100\n",
      "2793/2793 [==============================] - 0s 77us/step - loss: 0.0797 - acc: 0.2725 - val_loss: 0.0805 - val_acc: 0.2575\n",
      "Epoch 49/100\n",
      "2793/2793 [==============================] - 0s 77us/step - loss: 0.0796 - acc: 0.2710 - val_loss: 0.0804 - val_acc: 0.2718\n",
      "Epoch 50/100\n",
      "2793/2793 [==============================] - 0s 69us/step - loss: 0.0796 - acc: 0.2753 - val_loss: 0.0801 - val_acc: 0.2732\n",
      "Epoch 51/100\n",
      "2793/2793 [==============================] - 0s 72us/step - loss: 0.0795 - acc: 0.2764 - val_loss: 0.0800 - val_acc: 0.2704\n",
      "Epoch 52/100\n",
      "2793/2793 [==============================] - 0s 78us/step - loss: 0.0797 - acc: 0.2664 - val_loss: 0.0801 - val_acc: 0.2690\n",
      "Epoch 53/100\n",
      "2793/2793 [==============================] - 0s 73us/step - loss: 0.0795 - acc: 0.2671 - val_loss: 0.0801 - val_acc: 0.2732\n",
      "Epoch 54/100\n",
      "2793/2793 [==============================] - 0s 69us/step - loss: 0.0794 - acc: 0.2807 - val_loss: 0.0802 - val_acc: 0.2718\n",
      "Epoch 55/100\n",
      "2793/2793 [==============================] - 0s 74us/step - loss: 0.0795 - acc: 0.2778 - val_loss: 0.0803 - val_acc: 0.2432\n",
      "Epoch 56/100\n",
      "2793/2793 [==============================] - 0s 75us/step - loss: 0.0796 - acc: 0.2692 - val_loss: 0.0800 - val_acc: 0.2704\n",
      "Epoch 57/100\n",
      "2793/2793 [==============================] - 0s 85us/step - loss: 0.0796 - acc: 0.2678 - val_loss: 0.0802 - val_acc: 0.2704\n",
      "Epoch 58/100\n",
      "2793/2793 [==============================] - 0s 81us/step - loss: 0.0795 - acc: 0.2667 - val_loss: 0.0807 - val_acc: 0.2704\n",
      "Epoch 59/100\n",
      "2793/2793 [==============================] - 0s 77us/step - loss: 0.0796 - acc: 0.2689 - val_loss: 0.0803 - val_acc: 0.2690\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2793/2793 [==============================] - 0s 72us/step - loss: 0.0796 - acc: 0.2649 - val_loss: 0.0806 - val_acc: 0.2704\n",
      "Epoch 61/100\n",
      "2793/2793 [==============================] - 0s 79us/step - loss: 0.0796 - acc: 0.2678 - val_loss: 0.0802 - val_acc: 0.2532\n",
      "Epoch 62/100\n",
      "2793/2793 [==============================] - 0s 77us/step - loss: 0.0795 - acc: 0.2610 - val_loss: 0.0807 - val_acc: 0.2632\n",
      "Epoch 63/100\n",
      "2793/2793 [==============================] - 0s 80us/step - loss: 0.0797 - acc: 0.2760 - val_loss: 0.0803 - val_acc: 0.2704\n",
      "Epoch 64/100\n",
      "2793/2793 [==============================] - 0s 105us/step - loss: 0.0794 - acc: 0.2782 - val_loss: 0.0810 - val_acc: 0.2504\n",
      "Epoch 65/100\n",
      "2793/2793 [==============================] - 0s 96us/step - loss: 0.0795 - acc: 0.2750 - val_loss: 0.0805 - val_acc: 0.2704\n",
      "Epoch 66/100\n",
      "2793/2793 [==============================] - 0s 95us/step - loss: 0.0796 - acc: 0.2689 - val_loss: 0.0802 - val_acc: 0.2704\n",
      "Epoch 67/100\n",
      "2793/2793 [==============================] - 0s 110us/step - loss: 0.0795 - acc: 0.2768 - val_loss: 0.0804 - val_acc: 0.2732\n",
      "Epoch 68/100\n",
      "2793/2793 [==============================] - 0s 93us/step - loss: 0.0795 - acc: 0.2796 - val_loss: 0.0808 - val_acc: 0.2704\n",
      "Epoch 69/100\n",
      "2793/2793 [==============================] - 0s 109us/step - loss: 0.0796 - acc: 0.2757 - val_loss: 0.0804 - val_acc: 0.2718\n",
      "Epoch 70/100\n",
      "2793/2793 [==============================] - 0s 92us/step - loss: 0.0794 - acc: 0.2743 - val_loss: 0.0803 - val_acc: 0.2704\n",
      "Epoch 71/100\n",
      "2793/2793 [==============================] - 0s 112us/step - loss: 0.0794 - acc: 0.2721 - val_loss: 0.0807 - val_acc: 0.2704\n",
      "Epoch 72/100\n",
      "2793/2793 [==============================] - 0s 103us/step - loss: 0.0796 - acc: 0.2642 - val_loss: 0.0806 - val_acc: 0.2618\n",
      "Epoch 73/100\n",
      "2793/2793 [==============================] - 0s 93us/step - loss: 0.0795 - acc: 0.2657 - val_loss: 0.0807 - val_acc: 0.2604\n",
      "Epoch 74/100\n",
      "2793/2793 [==============================] - 0s 99us/step - loss: 0.0795 - acc: 0.2682 - val_loss: 0.0811 - val_acc: 0.2489\n",
      "Epoch 75/100\n",
      "2793/2793 [==============================] - 0s 98us/step - loss: 0.0795 - acc: 0.2689 - val_loss: 0.0807 - val_acc: 0.2690\n",
      "Epoch 76/100\n",
      "2793/2793 [==============================] - 0s 101us/step - loss: 0.0795 - acc: 0.2696 - val_loss: 0.0810 - val_acc: 0.2647\n",
      "Epoch 77/100\n",
      "2793/2793 [==============================] - 0s 88us/step - loss: 0.0796 - acc: 0.2610 - val_loss: 0.0809 - val_acc: 0.2632\n",
      "Epoch 78/100\n",
      "2793/2793 [==============================] - 0s 98us/step - loss: 0.0796 - acc: 0.2714 - val_loss: 0.0808 - val_acc: 0.2489\n",
      "Epoch 79/100\n",
      "2793/2793 [==============================] - 0s 100us/step - loss: 0.0795 - acc: 0.2728 - val_loss: 0.0807 - val_acc: 0.2489\n",
      "Epoch 80/100\n",
      "2793/2793 [==============================] - 0s 96us/step - loss: 0.0795 - acc: 0.2775 - val_loss: 0.0812 - val_acc: 0.2704\n",
      "Epoch 81/100\n",
      "2793/2793 [==============================] - 0s 91us/step - loss: 0.0795 - acc: 0.2803 - val_loss: 0.0813 - val_acc: 0.2504\n",
      "Epoch 82/100\n",
      "2793/2793 [==============================] - 0s 90us/step - loss: 0.0795 - acc: 0.2671 - val_loss: 0.0808 - val_acc: 0.2675\n",
      "Epoch 83/100\n",
      "2793/2793 [==============================] - 0s 96us/step - loss: 0.0795 - acc: 0.2657 - val_loss: 0.0810 - val_acc: 0.2690\n",
      "Epoch 84/100\n",
      "2793/2793 [==============================] - 0s 96us/step - loss: 0.0795 - acc: 0.2721 - val_loss: 0.0809 - val_acc: 0.2632\n",
      "Epoch 85/100\n",
      "2793/2793 [==============================] - 0s 97us/step - loss: 0.0795 - acc: 0.2653 - val_loss: 0.0810 - val_acc: 0.2704\n",
      "Epoch 86/100\n",
      "2793/2793 [==============================] - 0s 113us/step - loss: 0.0794 - acc: 0.2771 - val_loss: 0.0808 - val_acc: 0.2661\n",
      "Epoch 87/100\n",
      "2793/2793 [==============================] - 0s 105us/step - loss: 0.0797 - acc: 0.2692 - val_loss: 0.0806 - val_acc: 0.2690\n",
      "Epoch 88/100\n",
      "2793/2793 [==============================] - 0s 99us/step - loss: 0.0795 - acc: 0.2700 - val_loss: 0.0807 - val_acc: 0.2718\n",
      "Epoch 89/100\n",
      "2793/2793 [==============================] - 0s 88us/step - loss: 0.0795 - acc: 0.2721 - val_loss: 0.0808 - val_acc: 0.2504\n",
      "Epoch 90/100\n",
      "2793/2793 [==============================] - 0s 99us/step - loss: 0.0795 - acc: 0.2621 - val_loss: 0.0810 - val_acc: 0.2675\n",
      "Epoch 91/100\n",
      "2793/2793 [==============================] - 0s 91us/step - loss: 0.0794 - acc: 0.2778 - val_loss: 0.0812 - val_acc: 0.2704\n",
      "Epoch 92/100\n",
      "2793/2793 [==============================] - 0s 88us/step - loss: 0.0795 - acc: 0.2782 - val_loss: 0.0810 - val_acc: 0.2675\n",
      "Epoch 93/100\n",
      "2793/2793 [==============================] - 0s 83us/step - loss: 0.0794 - acc: 0.2671 - val_loss: 0.0813 - val_acc: 0.2661\n",
      "Epoch 94/100\n",
      "2793/2793 [==============================] - 0s 94us/step - loss: 0.0796 - acc: 0.2678 - val_loss: 0.0811 - val_acc: 0.2675\n",
      "Epoch 95/100\n",
      "2793/2793 [==============================] - 0s 90us/step - loss: 0.0796 - acc: 0.2714 - val_loss: 0.0811 - val_acc: 0.2489\n",
      "Epoch 96/100\n",
      "2793/2793 [==============================] - 0s 92us/step - loss: 0.0794 - acc: 0.2671 - val_loss: 0.0809 - val_acc: 0.2718\n",
      "Epoch 97/100\n",
      "2793/2793 [==============================] - 0s 98us/step - loss: 0.0795 - acc: 0.2757 - val_loss: 0.0809 - val_acc: 0.2718\n",
      "Epoch 98/100\n",
      "2793/2793 [==============================] - 0s 93us/step - loss: 0.0795 - acc: 0.2621 - val_loss: 0.0809 - val_acc: 0.2675\n",
      "Epoch 99/100\n",
      "2793/2793 [==============================] - 0s 100us/step - loss: 0.0796 - acc: 0.2649 - val_loss: 0.0811 - val_acc: 0.2675\n",
      "Epoch 100/100\n",
      "2793/2793 [==============================] - 0s 135us/step - loss: 0.0794 - acc: 0.2682 - val_loss: 0.0811 - val_acc: 0.2704\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e393ad47f0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_arousal.fit(X_training, Y_training_arousal, epochs = 100, verbose = 1, validation_data=(X_test, Y_test_arousal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare and choosing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can easily see that three models have very low accuracy on both of validation and training test because of lacking data\n",
    "#### It is very easy to see that the first model is very basic and easy to build but it might be underfitting\n",
    "#### Second model works quite fast and well. The accuracy of training and accuracy of testing are getting closer\n",
    "#### In third model, I added some big steps which are claimed to get better result but It seems to me not affect to our model and even make model run slower\n",
    "#### As the result, The second model is the best choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
